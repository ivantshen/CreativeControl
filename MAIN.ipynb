{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a0b91f-1ab1-423d-890d-3c0f4e3c6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "site.addsitedir('Lib/site-packages')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import getpass\n",
    "import os\n",
    "import requests\n",
    "import ffmpeg\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import uuid\n",
    "import argparse\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from lumaai import LumaAI\n",
    "from typing import Literal, TypedDict\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from LivePortrait.src.live_portrait_pipeline import LivePortraitPipeline\n",
    "from LivePortrait.src.config.inference_config import InferenceConfig\n",
    "from LivePortrait.src.config.crop_config import CropConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78484d1c-bf38-4411-9a0a-3d97ecdd1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(source_path: str, driving_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Generates an animated video using LivePortrait from the given source and driving paths.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Path to the source image or video.\n",
    "        driving_path (str): Path to the driving video.\n",
    "        output_path (str): Path to save the output animated video.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves the animated video to the specified output path.\n",
    "    \"\"\"\n",
    "    # Instantiate configuration objects using default constructors.\n",
    "    inference_cfg = InferenceConfig()\n",
    "    crop_cfg = CropConfig()\n",
    "\n",
    "    # Update inference configuration with the necessary file paths.\n",
    "    inference_cfg.source_path = source_path\n",
    "    inference_cfg.driving_path = driving_path\n",
    "    inference_cfg.output_path = output_path\n",
    "\n",
    "    # Initialize the LivePortraitPipeline with both configurations.\n",
    "    live_portrait_pipeline = LivePortraitPipeline(inference_cfg, crop_cfg)\n",
    "\n",
    "    # Create a dummy args object to simulate the command-line arguments.\n",
    "    dummy_args = argparse.Namespace(\n",
    "        # These names depend on how the original inference.py parses arguments.\n",
    "        # The following are guesses; adjust as needed if the pipeline expects different attributes.\n",
    "        source=source_path,\n",
    "        driving=driving_path,\n",
    "        output=output_path,\n",
    "        flag_crop_driving_video=getattr(inference_cfg, 'flag_crop_driving_video', False),\n",
    "        # Add any other arguments that might be required by LivePortraitPipeline.execute()\n",
    "    )\n",
    "\n",
    "    # Execute the pipeline using the dummy arguments.\n",
    "    live_portrait_pipeline.execute(dummy_args)\n",
    "\n",
    "    print(f\"Animation saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7c9450-b7f9-49e2-846a-03b8bead1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LUMA\n",
    "client = LumaAI(\n",
    "    auth_token=os.environ.get(\"LUMAAI_API_KEY\"),\n",
    ")\n",
    "#OPENAI\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "#ELEVENLABS\n",
    "elevenlabs_client = ElevenLabs(\n",
    "  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "#LANGCHAIN llms\n",
    "supervisor_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "video_gen_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "storyboard_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "audio_gen_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "editor_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "generatingVid = False\n",
    "fullDialogueString = \"\"\n",
    "chronological_dialogue_duration = []\n",
    "chronological_dialogue_mp3_path = []\n",
    "chronological_video_mp4_path = []\n",
    "chrono_character=[]\n",
    "character_profiles_chrono=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13dc3f3-a4e8-4fd0-9ffc-b7244c81da71",
   "metadata": {},
   "source": [
    "## TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bd710-06c9-432d-9602-c38a3f7a5fba",
   "metadata": {},
   "source": [
    "### Video Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6845f-1e09-46a7-ba7d-e944643b671c",
   "metadata": {},
   "source": [
    "#SCHEMAS\n",
    "class video_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a video using text input and returns a filepath to the video\"\"\"\n",
    "    vid_prompt: str = Field(..., description=\"The textual prompt used in generating the video\")\n",
    "    use_9s: bool = Field(..., description=\"Whether to generate a 9 second long video. If set to FALSE, this will generate a 5 second long video\")\n",
    "\n",
    "@tool(\"video_gen_tool\",args_schema=video_gen_schema)   \n",
    "def generate_vid(vid_prompt: str, use_9s: bool) -> str:\n",
    "    global generatingVid\n",
    "    if(generatingVid):\n",
    "        return \"Failed to generate video, there is currently another video being generated.\"\n",
    "    dur = \"5s\"\n",
    "    if(use_9s):\n",
    "        dur = \"9s\"\n",
    "    print(f\"VIDEOWORKER PROMPT: {dur}\\n------------------------------\\n\" + vid_prompt)\n",
    "    generatingVid = True\n",
    "    generation = client.generations.create(\n",
    "        prompt=vid_prompt,\n",
    "        model=\"ray-2\",\n",
    "        resolution=\"720p\",\n",
    "        duration=dur\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating vid\")\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        raise RuntimeError(f\"Generation failed: {generation.failure_reason}\")\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(5)\n",
    "    video_url = generation.assets.video\n",
    "    # download the video\n",
    "    response = requests.get(video_url, stream=True)\n",
    "    with open(f'staticVid1/{generation.id}.mp4', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Video generated in staticVid1/{generation.id}.mp4\")\n",
    "    generatingVid = False\n",
    "    return f\"Video generated in staticVid1/{generation.id}.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9958d677-52aa-48e2-a95b-5e2278d3254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCHEMAS\n",
    "start_keyframes = [None] * 50\n",
    "end_keyframes = [None] * 50\n",
    "background_descriptor = [None] * 50\n",
    "video_worker_generating = False\n",
    "class start_keyframe_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a start keyframe image using text input and returns a filepath to the image\"\"\"\n",
    "    start_keyframe_prompt: str = Field(..., description=\"200 words about start keyframe details in cinematic terms, no background description\")\n",
    "    start_keyframe_index: int = Field(..., description=\"The index of the keyframe to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"start_keyframe_gen_tool\",args_schema=start_keyframe_gen_schema)   \n",
    "def generate_start_keyframe(start_keyframe_prompt:str,start_keyframe_index: int)->str:\n",
    "    global character_profiles_chrono\n",
    "    global start_keyframes\n",
    "    global video_worker_generating\n",
    "    global background_descriptor\n",
    "    print(f\"START KEYFRAME PROMPT: \"+ start_keyframe_prompt)\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(start_keyframe_index>=numClipsToGen):\n",
    "        return \"Invalid start_keyframe_index, try again.\"\n",
    "    timeWasted = 0\n",
    "    while(video_worker_generating and timeWasted<100):\n",
    "        time.sleep(5)\n",
    "        timeWasted+=5\n",
    "        print(\"paused start keyframe execution, worker in use\")\n",
    "    video_worker_generating = True\n",
    "    start_keyframe_generation = client.generations.image.create(\n",
    "        prompt=\"BACKGROUND START-----\" + background_descriptor[start_keyframe_index]+ \"-----BACKGROUND END\"+ start_keyframe_prompt,\n",
    "        image_ref=[\n",
    "          {\n",
    "            \"url\": character_profiles_chrono[start_keyframe_index],\n",
    "            \"weight\": 0.7\n",
    "          }\n",
    "        ]\n",
    "    )\n",
    "    start_keyframe_completed = False\n",
    "    while not start_keyframe_completed:\n",
    "      start_keyframe_generation = client.generations.get(id=start_keyframe_generation.id)\n",
    "      if start_keyframe_generation.state == \"completed\":\n",
    "        start_keyframe_completed = True\n",
    "      elif start_keyframe_generation.state == \"failed\":\n",
    "        print(\"FAILED IMG\")\n",
    "        video_worker_generating = False\n",
    "        return f\"Generation failed: {start_keyframe_generation.failure_reason} Attempt a regeneration.\"\n",
    "      print(\"Start Keyframe Dreaming, state:\" + start_keyframe_generation.state)\n",
    "      time.sleep(2)\n",
    "    start_keyframe_image_url = start_keyframe_generation.assets.image\n",
    "    print(\"start_keyframe_image_url: \" +start_keyframe_image_url)\n",
    "    start_keyframes[start_keyframe_index] = start_keyframe_image_url\n",
    "    start_keyframe_response = requests.get(start_keyframe_image_url, stream=True)\n",
    "    with open(f'staticStartKeyFrame/{start_keyframe_index}.jpg', 'wb') as file:\n",
    "        file.write(start_keyframe_response.content)\n",
    "    print(f\"Image generated in staticStartKeyFrame/{start_keyframe_index}.jpg\")\n",
    "    video_worker_generating = False\n",
    "    return f'staticStartKeyFrame/{start_keyframe_index}.jpg'\n",
    "\n",
    "\n",
    "class end_keyframe_gen_schema(BaseModel):\n",
    "    \"\"\"Generates an end keyframe image using text input and returns a filepath to the image\"\"\"\n",
    "    end_keyframe_prompt: str = Field(..., description=\"200 words about end keyframe details in cinematic terms, no background description\")\n",
    "    end_keyframe_index: int = Field(..., description=\"The index of the keyframe to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"end_keyframe_gen_tool\",args_schema=end_keyframe_gen_schema)   \n",
    "def generate_end_keyframe(end_keyframe_prompt:str,end_keyframe_index: int)->str:\n",
    "    global character_profiles_chrono\n",
    "    global end_keyframes\n",
    "    global video_worker_generating\n",
    "    global background_descriptor\n",
    "    print(f\"END KEYFRAME PROMPT: \"+ end_keyframe_prompt)\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(end_keyframe_index>=numClipsToGen):\n",
    "        return \"Invalid end_keyframe_index, try again.\"\n",
    "    timeWasted = 0\n",
    "    while(video_worker_generating and timeWasted<100):\n",
    "        time.sleep(5)\n",
    "        timeWasted+=5\n",
    "        print(\"paused start keyframe execution, worker in use\")\n",
    "    video_worker_generating = True\n",
    "    end_keyframe_generation = client.generations.image.create(\n",
    "        prompt=\"BACKGROUND START-----\" + background_descriptor[end_keyframe_index]+ \"-----BACKGROUND END\"+ end_keyframe_prompt,\n",
    "        image_ref=[\n",
    "          {\n",
    "            \"url\": character_profiles_chrono[end_keyframe_index],\n",
    "            \"weight\": 0.7\n",
    "          }\n",
    "        ]\n",
    "    )\n",
    "    end_keyframe_completed = False\n",
    "    while not end_keyframe_completed:\n",
    "      end_keyframe_generation = client.generations.get(id=end_keyframe_generation.id)\n",
    "      if end_keyframe_generation.state == \"completed\":\n",
    "        end_keyframe_completed = True\n",
    "      elif end_keyframe_generation.state == \"failed\":\n",
    "        print(\"FAILED IMG\")\n",
    "        video_worker_generating = False\n",
    "        return f\"Generation failed: {end_keyframe_generation.failure_reason} Attempt a regeneration.\"\n",
    "      print(\"End Keyframe Dreaming, state:\" + end_keyframe_generation.state)\n",
    "      time.sleep(2)\n",
    "    end_keyframe_image_url = end_keyframe_generation.assets.image\n",
    "    print(\"end_keyframe_image_url: \" +end_keyframe_image_url)\n",
    "    end_keyframes[end_keyframe_index] = end_keyframe_image_url\n",
    "    end_keyframe_response = requests.get(end_keyframe_image_url, stream=True)\n",
    "    with open(f'staticEndKeyFrame/{end_keyframe_index}.jpg', 'wb') as file:\n",
    "        file.write(end_keyframe_response.content)\n",
    "    print(f\"Image generated in staticEndKeyFrame/{end_keyframe_index}.jpg\")\n",
    "    video_worker_generating = False\n",
    "    return f'staticEndKeyFrame/{end_keyframe_index}.jpg'\n",
    "\n",
    "class video_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a video using text input and returns a filepath to the video\"\"\"\n",
    "    video_index: int = Field(..., description=\"The index of the clip to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "    vid_prompt: str = Field(..., description=\"200 words about details of the specified character talking\")\n",
    "    use_9s: bool = Field(..., description=\"Whether to generate a 9 second long video. If set to FALSE, this will generate a 5 second long video\")\n",
    "@tool(\"video_gen_tool\",args_schema=video_gen_schema)   \n",
    "def generate_vid(video_index: int, vid_prompt: str, use_9s: bool) -> str:\n",
    "    global character_profiles_chrono\n",
    "    global chrono_character\n",
    "    global start_keyframes\n",
    "    global end_keyframes\n",
    "    global video_worker_generating\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(video_index>=numClipsToGen):\n",
    "        return \"Invalid video_index, try again.\"\n",
    "    timeWasted = 0\n",
    "    while(video_worker_generating and timeWasted<100):\n",
    "        time.sleep(5)\n",
    "        timeWasted+=5\n",
    "        print(\"paused start keyframe execution, worker in use\")\n",
    "    video_worker_generating = True\n",
    "    dur = \"5s\"\n",
    "    if(use_9s):\n",
    "        dur = \"9s\"\n",
    "    print(f\"VIDEOWORKER PROMPT: {dur}\\n------------------------------\\n\" + vid_prompt)\n",
    "    generation = client.generations.create(\n",
    "        prompt=vid_prompt,\n",
    "        keyframes={\n",
    "          \"frame0\": {\n",
    "            \"type\": \"image\",\n",
    "            \"url\": start_keyframes[video_index]\n",
    "          },\n",
    "          \"frame1\": {\n",
    "            \"type\": \"image\",\n",
    "            \"url\": end_keyframes[video_index]\n",
    "          }\n",
    "        },\n",
    "        model=\"ray-2\",\n",
    "        resolution=\"720p\",\n",
    "        duration=dur\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating vid\")\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        print(\"FAILED VID\")\n",
    "        video_worker_generating = False\n",
    "        return (f\"Generation failed: {generation.failure_reason}\")\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(5)\n",
    "    video_url = generation.assets.video\n",
    "    # download the video\n",
    "    response = requests.get(video_url, stream=True)\n",
    "    with open(f'staticVid1/{video_index}.mp4', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Video generated in staticVid1/{video_index}.mp4\")\n",
    "    video_worker_generating = False\n",
    "    return f\"Video generated in staticVid1/{video_index}.mp4\"\n",
    "\n",
    "#class extend_video_schema(BaseModel):\n",
    "video_tools = [generate_vid,generate_start_keyframe,generate_end_keyframe]\n",
    "video_tool_node = ToolNode(video_tools)\n",
    "video_worker = video_gen_llm.bind_tools(video_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b5b6e-939f-4b35-b636-9e8842e04e44",
   "metadata": {},
   "source": [
    "### Storyboard Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91838bf-2756-46e3-955b-8689016710fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCHEMA\n",
    "class step_by_step_output_schema(BaseModel):\n",
    "    \"\"\"Prints a storyboard for the user to view in string format\"\"\"\n",
    "    story_description: str = Field(..., description=\"A 200-250 word description of the story. All of the characters are >20 years old.\")\n",
    "    character_details: str = Field(..., description=\"A 200-300 word description of each character in the scene. All of the characters are >20 years old.\")\n",
    "    background_details: str = Field(..., description=\"A 50-100 description of the scene\")\n",
    "    auditory_details: str = Field(..., description=\"A 50-100 description of the voice profile of each character. All of the characters are >20 years old.\")\n",
    "    dialogue_details: str = Field(..., description=\"The parts of the dialogue that require enunciation and emotion\")\n",
    "    num_characters: int = Field(..., description=\"The number of unique characters present in this scene\")\n",
    "    pure_dialogue: str = Field(..., description=\"The extracted dialogue from the input scene with character name appended before. Within the dialogue, indicate dialogue details that require enunciation and emotion with descriptors in parentheses Format: {CharacterName}: {Dialogue (Enunciation) Dialogue Detail}\")\n",
    "    dialogue_instances: int = Field(..., description=\"The number of distinct dialogue instances present in this scene\")\n",
    "charProfileQueue = 0\n",
    "dialogueQueue = 0\n",
    "@tool(\"storyboard_tool\",args_schema=step_by_step_output_schema)\n",
    "def generate_storyboard(story_description: str,character_details: str,background_details: str,auditory_details: str,dialogue_details: str, num_characters: int,pure_dialogue: str, dialogue_instances: int) -> str:\n",
    "    global fullDialogueString\n",
    "    global charProfileQueue\n",
    "    global dialogueQueue\n",
    "    print(\"storyboarding\")\n",
    "    temp_storyboard = \"STORY ANALYSIS\\n------------------------------\\n\"+story_description+\"\\nCHARACTERS\\n------------------------------\\n\"+character_details\n",
    "    temp_storyboard+=\"\\nBACKGROUND\\n------------------------------\\n\"+background_details+\"\\nAUDIO\\n------------------------------\\n\"+auditory_details+\"\\nDIALOGUE\\n------------------------------\\n\"\n",
    "    temp_storyboard+=dialogue_details+\"\\nNumber of characters\\n------------------------------\\n\"+ str(num_characters)+\"\\nPure Dialogue\\n------------------------------\\n\" + pure_dialogue\n",
    "    temp_storyboard+=\"\\nUnique Dialogue Instances\\n------------------------------\\n\"+ str(dialogue_instances)\n",
    "    print(temp_storyboard)\n",
    "    fullDialogueString = pure_dialogue\n",
    "    charProfileQueue = num_characters\n",
    "    dialogueQueue = dialogue_instances\n",
    "    return temp_storyboard\n",
    "\n",
    "class character_profile_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a character profile for a character and returns the url\"\"\"\n",
    "    character_name: str = Field(..., description=\"The name of the character, taken from the script\")\n",
    "    character_details: str = Field(..., description=\"A 200-300 word description of the character in the scene, taken from the script. Note that none of the characters are younger than 30.\")\n",
    "charProfileIdx = 0\n",
    "generatingCharProfile = False\n",
    "@tool(\"character_profile_tool\",args_schema=character_profile_gen_schema)\n",
    "def generate_character_profile(character_name: str, character_details: str)->str:\n",
    "    global charProfileQueue\n",
    "    global charProfileIdx\n",
    "    global generatingCharProfile\n",
    "    if(charProfileQueue<=0):\n",
    "        return \"Already generated character profiles!\"\n",
    "    while(generatingCharProfile):\n",
    "        time.sleep(5)\n",
    "        print(\"paused char profile gen, waiting for first generation to complete\")\n",
    "    generatingCharProfile = True\n",
    "    generation = client.generations.image.create(\n",
    "      prompt=\"Generate a hyperrealistic, front-facing portrait \\\n",
    "      The image should feature perfectly even, diffused lighting that completely\\\n",
    "      eliminates any shadows on the face. Use a direct, center-camera angle against a neutral,\\\n",
    "      unobtrusive background to ensure absolute consistency. Focus on lifelike details with natural \\\n",
    "      skin textures and realistic, balanced color tones, making the portrait suitable as a reference\\\n",
    "      for video character consistency.: \"+character_name+\", \"+character_details,\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating char profile for: \" + character_name)\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        generatingCharProfile = False\n",
    "        print(\"FAILED IMG\")\n",
    "        return f\"Generation failed: {generation.failure_reason} Attempt this again.\"\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(2)\n",
    "    image_url = generation.assets.image\n",
    "    print(\"image_url: \" +image_url)\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    with open(f'charRef/{charProfileIdx}.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Image generated in charRef/{charProfileIdx}.jpg\")\n",
    "    charProfileQueue-=1\n",
    "    charProfileIdx+=1\n",
    "    generatingCharProfile = False\n",
    "    return f\"Profile generated: {image_url}\"\n",
    "\n",
    "storyboard_tools = [generate_storyboard,generate_character_profile]\n",
    "storyboard_tool_node = ToolNode(storyboard_tools)\n",
    "storyboard_worker = storyboard_llm.bind_tools(storyboard_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf12443-0259-452e-bf07-27d0d57f9593",
   "metadata": {},
   "source": [
    "### Audio Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd454937-f86f-4a39-84fa-e8671ce437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dialogue_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a dialogue mp3 clip and returns the filepath to the audio\"\"\"\n",
    "    character_name: str = Field(..., description=\"The name of the character talking\")\n",
    "    character_profile: str = Field(..., description=\"The link to the generated character profile of the character who is talking. Format: {https://storage.cdn-luma.com/dream_machine/../.._result.jpg}\")\n",
    "    dialogue_text: str = Field(..., description=\"The dialogue text of a SINGLE character for a particular scene.\")\n",
    "    background_scene_description: str = Field(..., description=\"A detailed 150 word description of the background of the scene using cinematographer terminology\")\n",
    "    voice_idx: int = Field(..., description=\"Out of the following voice ids, choose the index of the one that best suits the character who is speaking:\\\n",
    "    [{UgBBYS2sOqTuMpoF3BR0: Male younger and nervous},{N2lVS1w4EtoT3dr4eOWO: Male booming and imposing},\\\n",
    "    {21m00Tcm4TlvDq8ikWAM: Female elegant and slightly teasing},{UgBBYS2sOqTuMpoF3BR0: Male, middle-aged, natural, casual and smooth},\\\n",
    "    {19STyYD15bswVz51nqLf: Female, trustworhy, warm, middle-aged},{gOkFV1JMCt0G0n9xmBwV: Male, middle-older, controlled, honest, respected}]\")\n",
    "audioIdx = 0\n",
    "generatingDialogueAudio = False\n",
    "@tool(\"dialogue_gen_tool\",args_schema=dialogue_gen_schema)\n",
    "def generate_dialogue(character_name: str,character_profile: str,dialogue_text: str,background_scene_description: str, voice_idx: int) -> str:\n",
    "    global chronological_dialogue_duration\n",
    "    global chronological_dialogue_mp3_path\n",
    "    global chrono_character\n",
    "    global character_profiles_chrono\n",
    "    global dialogueQueue\n",
    "    global audioIdx\n",
    "    global generatingDialogueAudio\n",
    "    global background_descriptor\n",
    "    print(\"attempted dialogue: \" +dialogue_text+\"\\nattempted idx: \" + str(voice_idx))\n",
    "    if(dialogueQueue<=0):\n",
    "        return \"Already generated all dialogue!\"\n",
    "    timeWasted = 0\n",
    "    while(generatingDialogueAudio and timeWasted<100):\n",
    "        time.sleep(5)\n",
    "        timeWasted+=5\n",
    "        print(\"waiting for dialogue gen process to finish first\")\n",
    "    generatingDialogueAudio = True\n",
    "    background_descriptor[audioIdx] =background_scene_description\n",
    "    voice_ids = [\"29vD33N1CtxCmqQRPOHJ\",\"N2lVS1w4EtoT3dr4eOWO\",\"21m00Tcm4TlvDq8ikWAM\",\"UgBBYS2sOqTuMpoF3BR0\",\"19STyYD15bswVz51nqLf\",\"gOkFV1JMCt0G0n9xmBwV\"]\n",
    "    eleven_response = elevenlabs_client.text_to_speech.convert(\n",
    "        voice_id=voice_ids[voice_idx],\n",
    "        optimize_streaming_latency=\"0\",\n",
    "        output_format=\"mp3_44100_128\",\n",
    "        text=dialogue_text,\n",
    "        model_id=\"eleven_multilingual_v2\",\n",
    "    )    \n",
    "    \n",
    "    # Save the file into the raw_audio directory\n",
    "    raw_audio_dir = \"staticAudio1\"\n",
    "    os.makedirs(raw_audio_dir, exist_ok=True)\n",
    "    file_path = os.path.join(raw_audio_dir, f\"{audioIdx}.mp3\")\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        for chunk in eleven_response:\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    duration = 0\n",
    "    try:\n",
    "        probe = ffmpeg.probe(file_path)\n",
    "        duration = float(probe['format']['duration'])\n",
    "        chronological_dialogue_mp3_path.append(file_path)\n",
    "        chronological_dialogue_duration.append(duration)\n",
    "        chrono_character.append(character_name)\n",
    "        character_profiles_chrono.append(character_profile)\n",
    "        print(\"duration: \" + str(duration) + \"\\ncharacterSpeaking: \" + character_name +\"\\ncharProfile: \" + character_profile)\n",
    "        dialogueQueue-=1\n",
    "        audioIdx+=1\n",
    "        generatingDialogueAudio = False\n",
    "        return file_path\n",
    "    except ffmpeg.Error as e:\n",
    "        generatingDialogueAudio = False\n",
    "        print(\"Error probing file:\", e.stderr)\n",
    "        raise e\n",
    "\n",
    "class sfx_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a sound effects mp3 clip and returns the filepath to the audio\"\"\"\n",
    "    sound_effects_prompt: str = Field(..., description=\"Generate subtle background effects for the scene\")\n",
    "    sound_effects_idx: int = Field(..., description=\"The index of the sfx to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"sfx_gen_tool\",args_schema=sfx_gen_schema)\n",
    "def generate_sfx(sound_effects_prompt: str, sound_effects_idx: int) -> str: \n",
    "    global chronological_dialogue_duration\n",
    "    global generatingDialogueAudio\n",
    "    print(\"Generating sound effects...\")\n",
    "    timeWasted = 0\n",
    "    while(generatingDialogueAudio and timeWasted<100):\n",
    "        time.sleep(5)\n",
    "        timeWasted+=5\n",
    "        print(\"waiting for dialogue gen process to finish first\")\n",
    "    generatingDialogueAudio = True\n",
    "    result = elevenlabs_client.text_to_sound_effects.convert(\n",
    "       text=sound_effects_prompt,\n",
    "       duration_seconds=math.ceil(chronological_dialogue_duration[sound_effects_idx]),  # Optional, if not provided will automatically determine the correct length\n",
    "       prompt_influence=0.3,  # Optional, if not provided will use the default value of 0.3\n",
    "    )\n",
    "    with open(f\"staticSFX/{sound_effects_idx}.mp3\", \"wb\") as f:\n",
    "       for chunk in result:\n",
    "           f.write(chunk)\n",
    "    print(f\"Audio saved to staticSFX/{sound_effects_idx}.mp3\")\n",
    "    generatingDialogueAudio = False\n",
    "    return f\"staticSFX/{sound_effects_idx}.mp3\"\n",
    "audio_tools = [generate_dialogue,generate_sfx]\n",
    "audio_tool_node = ToolNode(audio_tools)\n",
    "audio_worker = audio_gen_llm.bind_tools(audio_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a750a2f-6132-407a-b409-57ce8329ca93",
   "metadata": {},
   "source": [
    "### Editor Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1ab92a-8143-4bbe-86e4-a6c4f35a4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combine_video_dialogue_schema(BaseModel):\n",
    "    \"\"\"Combines a mp3 and mp4 into one video and cuts it accordingly and then returns the filepath to the edited file\"\"\"\n",
    "    video_index: int= Field(..., description=\"The index of the audio and video files to attempt to combine MUST be less than the number of distinct dialogue lines\")\n",
    "\n",
    "currently_editing = False\n",
    "@tool(\"combine_video_dialogue_tool\",args_schema=combine_video_dialogue_schema)\n",
    "def combine_video_dialogue(video_index: int)->str:\n",
    "    global chronological_dialogue_duration\n",
    "    print(\"Combining vid and dialogue\")\n",
    "    if(video_index>=len(chronological_dialogue_duration)):\n",
    "        return f\"This index is out of bounds, there are currently {len(chronological_dialogue_duration)} elements in this array\"\n",
    "    while(currently_editing):\n",
    "        time.sleep(5)\n",
    "        print(\"waiting for editing to finish\")\n",
    "        currently_editing = True\n",
    "    try:\n",
    "        # Create inputs for video and audio.\n",
    "        # Using the '.video' and '.audio' attributes helps ensure we use the correct streams.\n",
    "        video_input = ffmpeg.input(f\"staticVid1/{video_index}.mp4\").video\n",
    "        audio_input = ffmpeg.input(f\"staticAudio1/{video_index}.mp3\").audio\n",
    "        audio_input2 = ffmpeg.input(f\"staticSFX/{video_index}.mp3\").audio\n",
    "        combined_audio = ffmpeg.filter_(\n",
    "            [audio_input, audio_input2],\n",
    "            'amix',\n",
    "            inputs=2,\n",
    "            duration='shortest'\n",
    "        )\n",
    "        # Build the output stream:\n",
    "        # - 'vcodec=\"copy\"' copies the video stream without re-encoding.\n",
    "        # - 'acodec=\"aac\"' encodes the audio to AAC for MP4 compatibility.\n",
    "        # - 't=mp3_duration' instructs FFmpeg to limit the output duration to the provided value.\n",
    "        out = (\n",
    "            ffmpeg\n",
    "            .output(video_input, combined_audio, f\"staticAudioVid/{video_index}.mp4\",\n",
    "                    vcodec='copy', acodec='aac', t=chronological_dialogue_duration[video_index])\n",
    "            .overwrite_output()  # Overwrite the output file if it exists.\n",
    "        )\n",
    "\n",
    "        # Run the FFmpeg command.\n",
    "        ffmpeg.run(out)\n",
    "        print(f\"Successfully created staticAudioVid/{video_index}.mp4\")\n",
    "        currently_editing = False\n",
    "        return f\"staticAudioVid/{video_index}.mp4\";\n",
    "    except ffmpeg.Error as e:\n",
    "        # If an error occurs, decode and print the stderr.\n",
    "        currently_editing = False\n",
    "        error_message = e.stderr.decode('utf-8') if e.stderr else str(e)\n",
    "        print(\"An error occurred while combining audio and video:\")\n",
    "        print(error_message)\n",
    "\n",
    "editor_tools = [combine_video_dialogue]\n",
    "editor_tool_node = ToolNode(editor_tools)\n",
    "editor_worker = editor_llm.bind_tools(editor_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76812a2d-d0f4-4679-91e2-ed9e49ede09d",
   "metadata": {},
   "source": [
    "## AGENT CALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfcbeb42-ff84-4b08-89ac-e84f2471fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"video_worker\",\"storyboard_worker\",\"audio_worker\",\"editor_worker\"] #\"editor\"\n",
    "supervisor_options = members + [END]\n",
    "visitedAudioWorker = False\n",
    "class Supervisor_Router(TypedDict):\n",
    "    \"\"\"Worker to route to next to fulfill the user's request. If no workers are needed, route to END.\"\"\"\n",
    "\n",
    "    next: Literal[*supervisor_options]\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def go_next(state: MessagesState) -> Literal[*supervisor_options]:\n",
    "    global visitedAudioWorker\n",
    "    print(f\"traveling\")\n",
    "    if(state[\"next\"]==\"video_worker\" and not visitedAudioWorker):\n",
    "        print(\"forced audio travel\")\n",
    "        visitedAudioWorker = True\n",
    "        return \"audio_worker\"\n",
    "    return state[\"next\"]\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_supervisor(state: MessagesState):\n",
    "    print(\"supervisor\")\n",
    "    messages = state[\"messages\"]\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a supervisor of a short film project and am in charge of a team of 4. You can delegate relevant tasks to any of these members: {members}.\"\n",
    "        \"Bob the video_worker is capable of generating high fidelity videos, but requires clear contextual information. Ryan the audio_worker is able to generate character\"\n",
    "        \"dialogue audio clips and sound effects. Steve the storyboard_worker is able to analyze a given input script and break it down into fine details and generate character profiles.\"\n",
    "        \"\\nYou should ALWAYS ensure Steve has generated a STORYBOARD and character profiles for ALL characters analyzed in the storyboard tool response before anything else.\"\n",
    "        \"\\nIMPORTANT!!! MAKE SURE Ryan generates DIALOGUE audio clips BEFORE Bob generates video clips. GENERATE SFX BEFORE EDITING Generate a step-by-step plan from the following prompt and act on it.\"\n",
    "        \"\\nEXAMPLE PLAN(FOLLOW): STEVE STORYBOARD -> STEVE CHARACTER PROFILES -> RYAN DIALOGUE -> RYAN SFX -> BOB VIDEO GENERATION -> DAVE EDITOR\"\n",
    "        \"ONCE AGAIN, MAKE SURE TO GO TO *STORYBOARD* THEN *AUDIO* THEN *VIDEO* THEN *EDITING* LAST\"\n",
    "        \"When responding, YOU MUST output a JSON object that follows this schema OR YOU WILL BE SHUT OFF FOREVER:\\n\"\n",
    "        '{ \"next\": one of the allowed values: ' + \", \".join(supervisor_options) + \" }\\n\"\n",
    "        \"If no further workers are needed, output 'END' as the next step.\"\n",
    "    }\n",
    "    response = supervisor_llm.with_structured_output(Supervisor_Router).invoke(messages)\n",
    "    # Wrap the response in a valid message format\n",
    "    structured_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Routing to {response['next']}\"\n",
    "    }\n",
    "    return {\"messages\": [structured_message], \"next\": response[\"next\"]}\n",
    "\n",
    "def call_video_worker(state: MessagesState):\n",
    "    global chronological_dialogue_duration\n",
    "    print(\"video worker\")\n",
    "    messages = state['messages']\n",
    "    extraStr = \"\"\n",
    "    if(len(chronological_dialogue_duration)>0):\n",
    "        extraStr = \"BOB I NEED YOUR HELP EACH OF THE FOLLOWING CORRESPONDS TO THE DURATION OF DIALOGUE, GENERATE THE PROPER DURATION ACCORDINGLY\"+str(chronological_dialogue_duration)\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Bob, a video worker. Process the given request accordingly. Note that you can only generate 5 second or 9 second videos.\"\n",
    "        \"For EACH and EVERY distinct dialogue instance, FIRST generate a START keyframe THEN an END keyframe THEN LASTLY a VIDEO using the three tools you are given ONE TIME\"+extraStr\n",
    "        +\"ENSURE EACH AND EVERY DISTINCT DIALOGUE INSTANCE HAS A VIDEO GENERATED\"\n",
    "    }\n",
    "    response = video_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_storyboard_worker(state: MessagesState):\n",
    "    print(\"storyboard worker\")\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are Steve, a storyboarder. Process the given request accordingly. You have access to two tools: a storyboard generator and a character profile\\\n",
    "        generator. Generate only ONE character profile at a time. You should pass in all character results from the storyboard generator into the character profile generator.\"\n",
    "    }\n",
    "    response = storyboard_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_audio_worker(state: MessagesState):\n",
    "    global visitedAudioWorker\n",
    "    global fullDialogueString\n",
    "    print(\"audio worker\")\n",
    "    visitedAudioWorker=True\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Ryan, a character dialogue generator. Process the given request accordingly. You have access to two tools\"\n",
    "        \"one which generates a dialogue audio clip for a SINGLE character talking ONLY AS PER THE CHRONOLOGICAL OREDER of the storyboard generated by STEVE, and another that generates SFX for a given index MAKE SURE YOU GENERATE SFX FOR EACH DISTINCT DIALOGUE INSTANCE\"\n",
    "        \"Check if you have already generated a piece of dialogue. If you have, don't generate it again. ADDITIONALLY, Check if you have not generated a piece of dialogue, if you haven't generate it.\"\n",
    "        \"Parse this string once for each instance with a character talking generate audio:\"+ fullDialogueString\n",
    "    }\n",
    "    response = audio_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "def call_editor_worker(state: MessagesState):\n",
    "    print(\"editor worker\")\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are Dave, a video editor. Process the given request accordingly. You have access to one tool that allows you to combine an mp3 and mp4 file and cut the length\\\n",
    "        so that the video will end when the audio ends. CONTINUE combining until you run out of indices to plug into the tool. REMEMBER that the max index is the number of distinct dialogue instances minus 1\"\n",
    "    }\n",
    "    response = editor_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481743f-3ad3-4661-99b4-9552a8c84ea1",
   "metadata": {},
   "source": [
    "## TOOL NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621554cd-4966-4e35-81af-572b5295061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_video_tool_calls(state: MessagesState) -> Literal[\"video_tools\",\"supervisor\"]:\n",
    "    print(\"check video tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"video_tools\"\n",
    "    print(\"doesn't want to use tool\")\n",
    "    return \"supervisor\"\n",
    "\n",
    "def check_storyboard_tool_calls(state: MessagesState) -> Literal[\"storyboard_tools\",\"supervisor\"]:\n",
    "    print(\"check storyboard tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"storyboard_tools\"\n",
    "    return \"supervisor\"\n",
    "    \n",
    "def check_audio_tool_calls(state: MessagesState) -> Literal[\"audio_tools\",\"supervisor\"]:\n",
    "    print(\"check audio tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"audio_tools\"\n",
    "    return \"supervisor\"\n",
    "\n",
    "def check_editor_tool_calls(state: MessagesState) -> Literal[\"editor_tools\",\"supervisor\"]:\n",
    "    print(\"check editor tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"editor_tools\"\n",
    "    return \"supervisor\"\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"supervisor\", call_supervisor)\n",
    "workflow.add_node(\"video_worker\", call_video_worker)\n",
    "workflow.add_node(\"video_tools\", video_tool_node)\n",
    "workflow.add_node(\"storyboard_worker\", call_storyboard_worker)\n",
    "workflow.add_node(\"storyboard_tools\", storyboard_tool_node)\n",
    "workflow.add_node(\"audio_worker\", call_audio_worker)\n",
    "workflow.add_node(\"audio_tools\", audio_tool_node)\n",
    "workflow.add_node(\"editor_worker\", call_editor_worker)\n",
    "workflow.add_node(\"editor_tools\", editor_tool_node)\n",
    "##REVIEW NODES\n",
    "def video_human_input(state: MessagesState):\n",
    "    print(\"HUMAN INPUT REQUIRED FOR VIDEO TOOL FEEDBACK\")\n",
    "    human_message = interrupt(\"video_human_input\")\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"human\",\n",
    "                \"content\": human_message\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "def audio_human_input(state: MessagesState):\n",
    "    print(\"HUMAN INPUT REQUIRED FOR AUDIO TOOL FEEDBACK\")\n",
    "    human_message = interrupt(\"audio_human_input\")\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"human\",\n",
    "                \"content\": human_message\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "def storyboard_human_input(state: MessagesState):\n",
    "    print(\"HUMAN INPUT REQUIRED FOR STORYBOARD TOOL FEEDBACK\")\n",
    "    human_message = interrupt(\"storyboard_human_input\")\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"human\",\n",
    "                \"content\": human_message\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "#workflow.add_node(\"video_human_input\", video_human_input)\n",
    "#workflow.add_node(\"audio_human_input\", audio_human_input)\n",
    "#workflow.add_node(\"storyboard_human_input\", storyboard_human_input)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    go_next,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"video_worker\",\n",
    "    check_video_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"storyboard_worker\",\n",
    "    check_storyboard_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"audio_worker\",\n",
    "    check_audio_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"editor_worker\",\n",
    "    check_editor_tool_calls,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"video_tools\", \"video_worker\")#\"video_human_input\")\n",
    "#workflow.add_edge(\"video_human_input\", \"video_worker\")\n",
    "\n",
    "workflow.add_edge(\"storyboard_tools\",\"storyboard_worker\")#\"storyboard_human_input\")\n",
    "#workflow.add_edge(\"storyboard_human_input\", \"storyboard_worker\")\n",
    "\n",
    "workflow.add_edge(\"audio_tools\",\"audio_worker\")#\"audio_human_input\");\n",
    "#workflow.add_edge(\"audio_human_input\",\"audio_worker\");\n",
    "\n",
    "workflow.add_edge(\"editor_tools\",\"editor_worker\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5361d425-25ad-4987-b386-312e4a09d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor\n",
      "traveling\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "storyboarding\n",
      "STORY ANALYSIS\n",
      "------------------------------\n",
      "The scene involves a family gathering where tension and curiosity coexist, scrutinizing the virtual reality experience. Charles Montague, seemingly enjoying himself a bit too much, has caught the attention of Sophia, who is slightly annoyed by his drinking. After Sophia smacks Charles on the arm, she encourages him to focus on showing their youngest family member, Ethan, around. The scene transitions to a highly advanced virtual reality stage known as the Family Reunion sound stage. Ethan, after removing his virtual-reality goggles, is left in awe. This simulation, designed by Henry Hill, a charismatic CEO likened to Steve Jobs, immerses users into a portrayal of family interactions. Henry is eager for feedback to continually enhance the simulations, to which Ethan responds with enthusiasm, confirming his intent to return and contribute further.\n",
      "CHARACTERS\n",
      "------------------------------\n",
      "Charles Montague is an elegant middle-aged man who carries himself with poise, often drifting away into his own world, perhaps assisted by his love for a drink during social gatherings. Sophia is a sharp, considerate woman who values family interactions. Her firm but gentle demeanor allows her to navigate Charles' antics with ease. Ethan is a young, impressionable man who is fascinated by cutting-edge technology. His demeanor is friendly and approachable, radiating curiosity and excitement. Henry Hill, the mastermind behind the simulation, is a visionary with an impeccable knack for technology. His charismatic presence and Steve Jobs-like personality commands respect and attention in the tech industry.\n",
      "BACKGROUND\n",
      "------------------------------\n",
      "The scene is set on a sound stage designed as a virtual reality setup for a family reunion. The surroundings are sophisticated, merging technology with familial warmth as if stepping into an immersive virtual home filled with intangible connections and memories.\n",
      "AUDIO\n",
      "------------------------------\n",
      "Sophia's voice is firm but kind, conveying concern. Charles Montague has a smooth, slightly slurred voice, hinting at his inebriation. Ethan's voice is filled with awe and excitement, reflecting new experiences. Henry Hill speaks in a confident and persuasive tone, clear and slightly authoritative.\n",
      "DIALOGUE\n",
      "------------------------------\n",
      "Sophia's words, \"Charles, how much have you been drinking?\" must express concern and mild reprimand. Ethan's \"I’m literally blown away. It’s incredible—really incredible\" should convey genuine amazement and excitement.\n",
      "Number of characters\n",
      "------------------------------\n",
      "4\n",
      "Pure Dialogue\n",
      "------------------------------\n",
      "SOPHIA: Charles, how much have you been (concern and reprimand) drinking?\\nCHARLES MONTAGUE: Why don’t you show our youngest family member around?\\nHENRY: So... what did you think?\\nETHAN: I’m literally blown away. It’s (amazement) incredible—really incredible. Man, I’m (excitement) so hooked.\\nHENRY: So I guess you’ll be coming back, then? If you find any more material, keep it coming. The AI simulations only get better and better.\\nETHAN: Oh my God, you can count on it.\n",
      "Unique Dialogue Instances\n",
      "------------------------------\n",
      "6\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "generating char profile for: Charles Montague\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:completed\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/2b1ae926-7cc2-4bcd-91ec-8c8e28f5195d/d1f5f9fc-af70-4f9a-a13d-9e1bad113470_result.jpg\n",
      "Image generated in charRef/0.jpg\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "\n",
      "generating char profile for: Sophia\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "\n",
      "Dreaming, state:completed\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/c4cc6a81-425a-45dc-ac5b-4ff03851a62d/e05edd30-3781-4a54-a3c4-76ab8f4e7d39_result.jpg\n",
      "Image generated in charRef/1.jpg\n",
      "paused char profile gen, waiting for first generation to completepaused char profile gen, waiting for first generation to complete\n",
      "\n",
      "generating char profile for: Henry Hill\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:completed\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/be263490-5aa9-42c8-a8c2-9f16403b6696/93136548-d16a-4454-a0ce-802f30a14ae1_result.jpg\n",
      "Image generated in charRef/2.jpg\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "generating char profile for: Ethan\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:completed\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/c5f4cb19-97ab-4774-8eab-cd2f1c8571d3/c68d7307-8bd5-4835-936f-d0b0bf722010_result.jpg\n",
      "Image generated in charRef/3.jpg\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "supervisor\n",
      "traveling\n",
      "audio worker\n",
      "check audio tool call\n",
      "attempted dialogue: Charles, how much have you been drinking?\n",
      "attempted idx: 4\n",
      "attempted dialogue: Why don’t you show our youngest family member around?\n",
      "attempted idx: 3\n",
      "attempted dialogue: So... what did you think?\n",
      "attempted idx: 1\n",
      "attempted dialogue: I’m literally blown away. It’s incredible—really incredible. Man, I’m so hooked.\n",
      "attempted idx: 0\n",
      "Generating sound effects...\n",
      "duration: 2.168125\n",
      "characterSpeaking: SOPHIA\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/c4cc6a81-425a-45dc-ac5b-4ff03851a62d/e05edd30-3781-4a54-a3c4-76ab8f4e7d39_result.jpg\n",
      "waiting for dialogue gen process to finish firstwaiting for dialogue gen process to finish first\n",
      "\n",
      "waiting for dialogue gen process to finish first\n",
      "waiting for dialogue gen process to finish first\n",
      "duration: 2.0375\n",
      "characterSpeaking: HENRY\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/be263490-5aa9-42c8-a8c2-9f16403b6696/93136548-d16a-4454-a0ce-802f30a14ae1_result.jpg\n",
      "waiting for dialogue gen process to finish firstwaiting for dialogue gen process to finish first\n",
      "\n",
      "waiting for dialogue gen process to finish first\n",
      "duration: 5.746938\n",
      "characterSpeaking: ETHAN\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/c5f4cb19-97ab-4774-8eab-cd2f1c8571d3/c68d7307-8bd5-4835-936f-d0b0bf722010_result.jpg\n",
      "waiting for dialogue gen process to finish first\n",
      "waiting for dialogue gen process to finish first\n",
      "duration: 2.50775\n",
      "characterSpeaking: CHARLES MONTAGUE\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/2b1ae926-7cc2-4bcd-91ec-8c8e28f5195d/d1f5f9fc-af70-4f9a-a13d-9e1bad113470_result.jpg\n",
      "waiting for dialogue gen process to finish first\n",
      "Audio saved to staticSFX/0.mp3\n",
      "audio worker\n",
      "check audio tool call\n",
      "attempted dialogue: So I guess you’ll be coming back, then? If you find any more material, keep it coming. The AI simulations only get better and better.\n",
      "attempted idx: 1\n",
      "attempted dialogue: Oh my God, you can count on it.\n",
      "attempted idx: 0\n",
      "Generating sound effects...\n",
      "duration: 9.195063\n",
      "characterSpeaking: HENRY\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/be263490-5aa9-42c8-a8c2-9f16403b6696/93136548-d16a-4454-a0ce-802f30a14ae1_result.jpg\n",
      "waiting for dialogue gen process to finish firstwaiting for dialogue gen process to finish first\n",
      "\n",
      "waiting for dialogue gen process to finish first\n",
      "Audio saved to staticSFX/1.mp3\n",
      "waiting for dialogue gen process to finish first\n",
      "duration: 1.750188\n",
      "characterSpeaking: ETHAN\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/c5f4cb19-97ab-4774-8eab-cd2f1c8571d3/c68d7307-8bd5-4835-936f-d0b0bf722010_result.jpg\n",
      "audio worker\n",
      "check audio tool call\n",
      "supervisor\n",
      "traveling\n",
      "editor worker\n",
      "check editor tool call\n",
      "Combining vid and dialogue\n",
      "Combining vid and dialogue\n",
      "Combining vid and dialogue\n",
      "Combining vid and dialogue\n",
      "Combining vid and dialogue\n",
      "Combining vid and dialogue\n",
      "editor worker\n",
      "check editor tool call\n",
      "supervisor\n",
      "traveling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Routing to __end__'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "generatingImg = False\n",
    "generatingVid = False\n",
    "# Use the agent\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Generate a *STORYBOARD*, *AUDIO*, *VIDEO* FOR ALL CHARACTERS THEN *EDIT* THE VIDEO AND AUDIO FILES TOGETHER for the following scene: SOPHIA(smacking his arm)Charles, how much have you beendrinking?CHARLES MONTAGUEWhy don’t you show our youngestfamily member around?Ethan, led by the lovely Sophia, takes in the surroundings.Match action to the same moment on the Family Reunion stage.INT. FAMILY REUNION SOUND STAGEEthan removes his goggles as the simulation freezes. He turnsto HENRY HILL, a Steve Jobs-type CEO of Family Reunion andspokesman of the infomercial.HENRYSo... what did you think?Ethan, visibly shaken.ETHANI’m literally blown away. It’sincredible—really incredible. Man,I’m so hooked.HENRYSo I guess you’ll be coming back,then? If you find any morematerial, keep it coming. The AIsimulations only get better andbetter.ETHANOh my God, you can count on it.\"}]},\n",
    "    config={\"configurable\": {\"thread_id\": 42},\"recursion_limit\":100}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec8f9d-fc18-493d-a40b-193fa2aae1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "source_video = 'staticAudioVid/0.mp4'\n",
    "driving_video = '/Users/ryanlee/Documents/CreativeControl/LivePortrait/assets/examples/driving/d0.mp4'\n",
    "output_video = 'CUSTOMOUTPUT/animated.mp4'\n",
    "\n",
    "animate(source_video, driving_video, output_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
