{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a0b91f-1ab1-423d-890d-3c0f4e3c6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "site.addsitedir('Lib/site-packages')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import getpass\n",
    "import os\n",
    "import requests\n",
    "import ffmpeg\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import uuid\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from lumaai import LumaAI\n",
    "from typing import Literal, TypedDict\n",
    "from elevenlabs.client import ElevenLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7c9450-b7f9-49e2-846a-03b8bead1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LUMA\n",
    "client = LumaAI(\n",
    "    auth_token=os.environ.get(\"LUMAAI_API_KEY\"),\n",
    ")\n",
    "#OPENAI\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "#ELEVENLABS\n",
    "elevenlabs_client = ElevenLabs(\n",
    "  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "#LANGCHAIN llms\n",
    "supervisor_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "video_gen_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "storyboard_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "audio_gen_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "editor_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "generatingVid = False\n",
    "fullDialogueString = \"\"\n",
    "chronological_dialogue_duration = []\n",
    "chronological_dialogue_mp3_path = []\n",
    "chronological_video_mp4_path = []\n",
    "chrono_character=[]\n",
    "character_profiles_chrono=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13dc3f3-a4e8-4fd0-9ffc-b7244c81da71",
   "metadata": {},
   "source": [
    "## TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bd710-06c9-432d-9602-c38a3f7a5fba",
   "metadata": {},
   "source": [
    "### Video Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6845f-1e09-46a7-ba7d-e944643b671c",
   "metadata": {},
   "source": [
    "#SCHEMAS\n",
    "class video_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a video using text input and returns a filepath to the video\"\"\"\n",
    "    vid_prompt: str = Field(..., description=\"The textual prompt used in generating the video\")\n",
    "    use_9s: bool = Field(..., description=\"Whether to generate a 9 second long video. If set to FALSE, this will generate a 5 second long video\")\n",
    "\n",
    "@tool(\"video_gen_tool\",args_schema=video_gen_schema)   \n",
    "def generate_vid(vid_prompt: str, use_9s: bool) -> str:\n",
    "    global generatingVid\n",
    "    if(generatingVid):\n",
    "        return \"Failed to generate video, there is currently another video being generated.\"\n",
    "    dur = \"5s\"\n",
    "    if(use_9s):\n",
    "        dur = \"9s\"\n",
    "    print(f\"VIDEOWORKER PROMPT: {dur}\\n------------------------------\\n\" + vid_prompt)\n",
    "    generatingVid = True\n",
    "    generation = client.generations.create(\n",
    "        prompt=vid_prompt,\n",
    "        model=\"ray-2\",\n",
    "        resolution=\"720p\",\n",
    "        duration=dur\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating vid\")\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        raise RuntimeError(f\"Generation failed: {generation.failure_reason}\")\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(5)\n",
    "    video_url = generation.assets.video\n",
    "    # download the video\n",
    "    response = requests.get(video_url, stream=True)\n",
    "    with open(f'staticVid1/{generation.id}.mp4', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Video generated in staticVid1/{generation.id}.mp4\")\n",
    "    generatingVid = False\n",
    "    return f\"Video generated in staticVid1/{generation.id}.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9958d677-52aa-48e2-a95b-5e2278d3254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCHEMAS\n",
    "start_keyframes = [None] * 50\n",
    "end_keyframes = [None] * 50\n",
    "class start_keyframe_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a start keyframe image using text input and returns a filepath to the image\"\"\"\n",
    "    start_keyframe_prompt: str = Field(..., description=\"200 words about start keyframe details\")\n",
    "    start_keyframe_index: int = Field(..., description=\"The index of the keyframe to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"start_keyframe_gen_tool\",args_schema=start_keyframe_gen_schema)   \n",
    "def generate_start_keyframe(start_keyframe_prompt:str,start_keyframe_index: int)->str:\n",
    "    global character_profiles_chrono\n",
    "    global start_keyframes\n",
    "    print(f\"START KEYFRAME PROMPT: \"+ start_keyframe_prompt)\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(start_keyframe_index>=numClipsToGen):\n",
    "        return \"Invalid start_keyframe_index, try again.\"\n",
    "    start_keyframe_generation = client.generations.image.create(\n",
    "        prompt=start_keyframe_prompt,\n",
    "        image_ref=[\n",
    "          {\n",
    "            \"url\": character_profiles_chrono[start_keyframe_index],\n",
    "            \"weight\": 0.7\n",
    "          }\n",
    "        ]\n",
    "    )\n",
    "    start_keyframe_completed = False\n",
    "    while not start_keyframe_completed:\n",
    "      start_keyframe_generation = client.generations.get(id=start_keyframe_generation.id)\n",
    "      if start_keyframe_generation.state == \"completed\":\n",
    "        start_keyframe_completed = True\n",
    "      elif start_keyframe_generation.state == \"failed\":\n",
    "        print(\"FAILED IMG\")\n",
    "        return f\"Generation failed: {start_keyframe_generation.failure_reason} Attempt a regeneration.\"\n",
    "      print(\"Start Keyframe Dreaming, state:\" + start_keyframe_generation.state)\n",
    "      time.sleep(2)\n",
    "    start_keyframe_image_url = start_keyframe_generation.assets.image\n",
    "    print(\"start_keyframe_image_url: \" +start_keyframe_image_url)\n",
    "    start_keyframes[start_keyframe_index] = start_keyframe_image_url\n",
    "    start_keyframe_response = requests.get(start_keyframe_image_url, stream=True)\n",
    "    with open(f'staticStartKeyFrame/{start_keyframe_index}.jpg', 'wb') as file:\n",
    "        file.write(start_keyframe_response.content)\n",
    "    print(f\"Image generated in staticStartKeyFrame/{start_keyframe_index}.jpg\")\n",
    "    return f'staticStartKeyFrame/{start_keyframe_index}.jpg'\n",
    "\n",
    "\n",
    "class end_keyframe_gen_schema(BaseModel):\n",
    "    \"\"\"Generates an end keyframe image using text input and returns a filepath to the image\"\"\"\n",
    "    end_keyframe_prompt: str = Field(..., description=\"200 words about end keyframe details\")\n",
    "    end_keyframe_index: int = Field(..., description=\"The index of the keyframe to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"end_keyframe_gen_tool\",args_schema=end_keyframe_gen_schema)   \n",
    "def generate_end_keyframe(end_keyframe_prompt:str,end_keyframe_index: int)->str:\n",
    "    global character_profiles_chrono\n",
    "    global end_keyframes\n",
    "    print(f\"END KEYFRAME PROMPT: \"+ end_keyframe_prompt)\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(end_keyframe_index>=numClipsToGen):\n",
    "        return \"Invalid end_keyframe_index, try again.\"\n",
    "    end_keyframe_generation = client.generations.image.create(\n",
    "        prompt=end_keyframe_prompt,\n",
    "        image_ref=[\n",
    "          {\n",
    "            \"url\": character_profiles_chrono[end_keyframe_index],\n",
    "            \"weight\": 0.7\n",
    "          }\n",
    "        ]\n",
    "    )\n",
    "    end_keyframe_completed = False\n",
    "    while not end_keyframe_completed:\n",
    "      end_keyframe_generation = client.generations.get(id=end_keyframe_generation.id)\n",
    "      if end_keyframe_generation.state == \"completed\":\n",
    "        end_keyframe_completed = True\n",
    "      elif end_keyframe_generation.state == \"failed\":\n",
    "        print(\"FAILED IMG\")\n",
    "        return f\"Generation failed: {end_keyframe_generation.failure_reason} Attempt a regeneration.\"\n",
    "      print(\"End Keyframe Dreaming, state:\" + end_keyframe_generation.state)\n",
    "      time.sleep(2)\n",
    "    end_keyframe_image_url = end_keyframe_generation.assets.image\n",
    "    print(\"end_keyframe_image_url: \" +end_keyframe_image_url)\n",
    "    end_keyframes[end_keyframe_index] = end_keyframe_image_url\n",
    "    end_keyframe_response = requests.get(end_keyframe_image_url, stream=True)\n",
    "    with open(f'staticEndKeyFrame/{end_keyframe_index}.jpg', 'wb') as file:\n",
    "        file.write(end_keyframe_response.content)\n",
    "    print(f\"Image generated in staticEndKeyFrame/{end_keyframe_index}.jpg\")\n",
    "    return f'staticEndKeyFrame/{end_keyframe_index}.jpg'\n",
    "\n",
    "class video_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a video using text input and returns a filepath to the video\"\"\"\n",
    "    video_index: int = Field(..., description=\"The index of the clip to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "    vid_prompt: str = Field(..., description=\"200 words about details of the specified character talking\")\n",
    "    use_9s: bool = Field(..., description=\"Whether to generate a 9 second long video. If set to FALSE, this will generate a 5 second long video\")\n",
    "@tool(\"video_gen_tool\",args_schema=video_gen_schema)   \n",
    "def generate_vid(video_index: int, vid_prompt: str, use_9s: bool) -> str:\n",
    "    global character_profiles_chrono\n",
    "    global chrono_character\n",
    "    global start_keyframes\n",
    "    global end_keyframes\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(video_index>=numClipsToGen):\n",
    "        return \"Invalid video_index, try again.\"\n",
    "    dur = \"5s\"\n",
    "    if(use_9s):\n",
    "        dur = \"9s\"\n",
    "    print(f\"VIDEOWORKER PROMPT: {dur}\\n------------------------------\\n\" + vid_prompt)\n",
    "    generation = client.generations.create(\n",
    "        prompt=vid_prompt,\n",
    "        keyframes={\n",
    "          \"frame0\": {\n",
    "            \"type\": \"image\",\n",
    "            \"url\": start_keyframes[video_index]\n",
    "          },\n",
    "          \"frame1\": {\n",
    "            \"type\": \"image\",\n",
    "            \"url\": end_keyframes[video_index]\n",
    "          }\n",
    "        },\n",
    "        model=\"ray-2\",\n",
    "        resolution=\"720p\",\n",
    "        duration=dur\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating vid\")\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        print(\"FAILED VID\")\n",
    "        return (f\"Generation failed: {generation.failure_reason}\")\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(5)\n",
    "    video_url = generation.assets.video\n",
    "    # download the video\n",
    "    response = requests.get(video_url, stream=True)\n",
    "    with open(f'staticVid1/{video_index}.mp4', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Video generated in staticVid1/{video_index}.mp4\")\n",
    "    return f\"Video generated in staticVid1/{video_index}.mp4\"\n",
    "\n",
    "#class extend_video_schema(BaseModel):\n",
    "video_tools = [generate_vid,generate_start_keyframe,generate_end_keyframe]\n",
    "video_tool_node = ToolNode(video_tools)\n",
    "video_worker = video_gen_llm.bind_tools(video_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b5b6e-939f-4b35-b636-9e8842e04e44",
   "metadata": {},
   "source": [
    "### Storyboard Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91838bf-2756-46e3-955b-8689016710fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCHEMA\n",
    "class step_by_step_output_schema(BaseModel):\n",
    "    \"\"\"Prints a storyboard for the user to view in string format\"\"\"\n",
    "    story_description: str = Field(..., description=\"A 200-250 word description of the story\")\n",
    "    character_details: str = Field(..., description=\"A 200-300 word description of each character in the scene\")\n",
    "    background_details: str = Field(..., description=\"A 50-100 description of the scene\")\n",
    "    auditory_details: str = Field(..., description=\"A 50-100 description of the voice profile of each character\")\n",
    "    dialogue_details: str = Field(..., description=\"The parts of the dialogue that require enunciation and emotion\")\n",
    "    num_characters: int = Field(..., description=\"The number of unique characters present in this scene\")\n",
    "    pure_dialogue: str = Field(..., description=\"The extracted dialogue from the input scene with character name appended before. Within the dialogue, indicate dialogue details that require enunciation and emotion with descriptors in parentheses Format: {CharacterName}: {Dialogue (Enunciation) Dialogue Detail}\")\n",
    "    dialogue_instances: int = Field(..., description=\"The number of distinct dialogue instances present in this scene\")\n",
    "charProfileQueue = 0\n",
    "dialogueQueue = 0\n",
    "@tool(\"storyboard_tool\",args_schema=step_by_step_output_schema)\n",
    "def generate_storyboard(story_description: str,character_details: str,background_details: str,auditory_details: str,dialogue_details: str, num_characters: int,pure_dialogue: str, dialogue_instances: int) -> str:\n",
    "    global fullDialogueString\n",
    "    global charProfileQueue\n",
    "    global dialogueQueue\n",
    "    print(\"storyboarding\")\n",
    "    temp_storyboard = \"STORY ANALYSIS\\n------------------------------\\n\"+story_description+\"\\nCHARACTERS\\n------------------------------\\n\"+character_details\n",
    "    temp_storyboard+=\"\\nBACKGROUND\\n------------------------------\\n\"+background_details+\"\\nAUDIO\\n------------------------------\\n\"+auditory_details+\"\\nDIALOGUE\\n------------------------------\\n\"\n",
    "    temp_storyboard+=dialogue_details+\"\\nNumber of characters\\n------------------------------\\n\"+ str(num_characters)+\"\\nPure Dialogue\\n------------------------------\\n\" + pure_dialogue\n",
    "    temp_storyboard+=\"\\nUnique Dialogue Instances\\n------------------------------\\n\"+ str(dialogue_instances)\n",
    "    print(temp_storyboard)\n",
    "    fullDialogueString = pure_dialogue\n",
    "    charProfileQueue = num_characters\n",
    "    dialogueQueue = dialogue_instances\n",
    "    return temp_storyboard\n",
    "\n",
    "class character_profile_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a character profile for a character and returns the url\"\"\"\n",
    "    character_name: str = Field(..., description=\"The name of the character, taken from the script\")\n",
    "    character_details: str = Field(..., description=\"A 200-300 word description of the character in the scene, taken from the script\")\n",
    "charProfileIdx = 0\n",
    "generatingCharProfile = False\n",
    "@tool(\"character_profile_tool\",args_schema=character_profile_gen_schema)\n",
    "def generate_character_profile(character_name: str, character_details: str)->str:\n",
    "    global charProfileQueue\n",
    "    global charProfileIdx\n",
    "    global generatingCharProfile\n",
    "    if(charProfileQueue<=0):\n",
    "        return \"Already generated character profiles!\"\n",
    "    while(generatingCharProfile):\n",
    "        time.sleep(5)\n",
    "        print(\"paused char profile gen, waiting for first generation to complete\")\n",
    "    generatingCharProfile = True\n",
    "    generation = client.generations.image.create(\n",
    "      prompt=\"Generate a hyperrealistic, front-facing portrait \\\n",
    "      The image should feature perfectly even, diffused lighting that completely\\\n",
    "      eliminates any shadows on the face. Use a direct, center-camera angle against a neutral,\\\n",
    "      unobtrusive background to ensure absolute consistency. Focus on lifelike details with natural \\\n",
    "      skin textures and realistic, balanced color tones, making the portrait suitable as a reference\\\n",
    "      for video character consistency.: \"+character_name+\", \"+character_details,\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating char profile for: \" + character_name)\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        generatingCharProfile = False\n",
    "        print(\"FAILED IMG\")\n",
    "        return f\"Generation failed: {generation.failure_reason} Attempt this again.\"\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(2)\n",
    "    image_url = generation.assets.image\n",
    "    print(\"image_url: \" +image_url)\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    with open(f'charRef/{charProfileIdx}.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Image generated in charRef/{charProfileIdx}.jpg\")\n",
    "    charProfileQueue-=1\n",
    "    charProfileIdx+=1\n",
    "    generatingCharProfile = False\n",
    "    return f\"Profile generated: {image_url}\"\n",
    "\n",
    "storyboard_tools = [generate_storyboard,generate_character_profile]\n",
    "storyboard_tool_node = ToolNode(storyboard_tools)\n",
    "storyboard_worker = storyboard_llm.bind_tools(storyboard_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf12443-0259-452e-bf07-27d0d57f9593",
   "metadata": {},
   "source": [
    "### Audio Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd454937-f86f-4a39-84fa-e8671ce437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dialogue_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a dialogue mp3 clip and returns the filepath to the audio\"\"\"\n",
    "    character_name: str = Field(..., description=\"The name of the character talking\")\n",
    "    character_profile: str = Field(..., description=\"The link to the generated character profile of the character who is talking. Format: {https://storage.cdn-luma.com/dream_machine/../.._result.jpg}\")\n",
    "    dialogue_text: str = Field(..., description=\"The dialogue text of a SINGLE character for a particular scene.\")\n",
    "    voice_idx: int = Field(..., description=\"Out of the following voice ids, choose the index of the one that best suits the character who is speaking:\\\n",
    "    [{UgBBYS2sOqTuMpoF3BR0: Male younger and nervous},{N2lVS1w4EtoT3dr4eOWO: Male booming and imposing},\\\n",
    "    {21m00Tcm4TlvDq8ikWAM: Female elegant and slightly teasing},{UgBBYS2sOqTuMpoF3BR0: Male, middle-aged, natural, casual and smooth},\\\n",
    "    {19STyYD15bswVz51nqLf: Female, trustworhy, warm, middle-aged},{gOkFV1JMCt0G0n9xmBwV: Male, middle-older, controlled, honest, respected}]\")\n",
    "audioIdx = 0\n",
    "generatingDialogueAudio = False\n",
    "@tool(\"dialogue_gen_tool\",args_schema=dialogue_gen_schema)\n",
    "def generate_dialogue(character_name: str,character_profile: str,dialogue_text: str, voice_idx: int) -> str:\n",
    "    global chronological_dialogue_duration\n",
    "    global chronological_dialogue_mp3_path\n",
    "    global chrono_character\n",
    "    global character_profiles_chrono\n",
    "    global dialogueQueue\n",
    "    global audioIdx\n",
    "    global generatingDialogueAudio\n",
    "    print(\"attempted dialogue: \" +dialogue_text+\"\\nattempted idx: \" + str(voice_idx))\n",
    "    if(dialogueQueue<=0):\n",
    "        return \"Already generated all dialogue!\"\n",
    "    while(generatingDialogueAudio):\n",
    "        time.sleep(5)\n",
    "        print(\"waiting for dialogue gen process to finish first\")\n",
    "    generatingDialogueAudio = True\n",
    "    voice_ids = [\"29vD33N1CtxCmqQRPOHJ\",\"N2lVS1w4EtoT3dr4eOWO\",\"21m00Tcm4TlvDq8ikWAM\",\"UgBBYS2sOqTuMpoF3BR0\",\"19STyYD15bswVz51nqLf\",\"gOkFV1JMCt0G0n9xmBwV\"]\n",
    "    eleven_response = elevenlabs_client.text_to_speech.convert(\n",
    "        voice_id=voice_ids[voice_idx],\n",
    "        optimize_streaming_latency=\"0\",\n",
    "        output_format=\"mp3_44100_128\",\n",
    "        text=dialogue_text,\n",
    "        model_id=\"eleven_multilingual_v2\",\n",
    "    )\n",
    "    \n",
    "    # Save the file into the raw_audio directory\n",
    "    raw_audio_dir = \"staticAudio1\"\n",
    "    os.makedirs(raw_audio_dir, exist_ok=True)\n",
    "    file_path = os.path.join(raw_audio_dir, f\"{audioIdx}.mp3\")\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        for chunk in eleven_response:\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    duration = 0\n",
    "    try:\n",
    "        probe = ffmpeg.probe(file_path)\n",
    "        duration = float(probe['format']['duration'])\n",
    "        chronological_dialogue_mp3_path.append(file_path)\n",
    "        chronological_dialogue_duration.append(duration)\n",
    "        chrono_character.append(character_name)\n",
    "        character_profiles_chrono.append(character_profile)\n",
    "        print(\"duration: \" + str(duration) + \"\\ncharacterSpeaking: \" + character_name +\"\\ncharProfile: \" + character_profile)\n",
    "        dialogueQueue-=1\n",
    "        audioIdx+=1\n",
    "        generatingDialogueAudio = False\n",
    "        return file_path\n",
    "    except ffmpeg.Error as e:\n",
    "        generatingDialogueAudio = False\n",
    "        print(\"Error probing file:\", e.stderr)\n",
    "        raise e\n",
    "    \n",
    "audio_tools = [generate_dialogue]\n",
    "audio_tool_node = ToolNode(audio_tools)\n",
    "audio_worker = audio_gen_llm.bind_tools(audio_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a750a2f-6132-407a-b409-57ce8329ca93",
   "metadata": {},
   "source": [
    "### Editor Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1ab92a-8143-4bbe-86e4-a6c4f35a4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combine_video_dialogue_schema(BaseModel):\n",
    "    \"\"\"Combines a mp3 and mp4 into one video and cuts it accordingly and then returns the filepath to the edited file\"\"\"\n",
    "    video_index: int= Field(..., description=\"The index of the audio and video files to attempt to combine MUST be less than the number of distinct dialogue lines\")\n",
    "\n",
    "@tool(\"combine_video_dialogue_tool\",args_schema=combine_video_dialogue_schema)\n",
    "def combine_video_dialogue(video_index: int)->str:\n",
    "    global chronological_dialogue_duration\n",
    "    print(\"Combining vid and dialogue\")\n",
    "    if(video_index>=len(chronological_dialogue_duration)):\n",
    "        return f\"This index is out of bounds, there are currently {len(chronological_dialogue_duration)} elements in this array\"\n",
    "    try:\n",
    "        # Create inputs for video and audio.\n",
    "        # Using the '.video' and '.audio' attributes helps ensure we use the correct streams.\n",
    "        video_input = ffmpeg.input(f\"staticVid1/{video_index}.mp4\").video\n",
    "        audio_input = ffmpeg.input(f\"staticAudio1/{video_index}.mp3\").audio\n",
    "\n",
    "        # Build the output stream:\n",
    "        # - 'vcodec=\"copy\"' copies the video stream without re-encoding.\n",
    "        # - 'acodec=\"aac\"' encodes the audio to AAC for MP4 compatibility.\n",
    "        # - 't=mp3_duration' instructs FFmpeg to limit the output duration to the provided value.\n",
    "        out = (\n",
    "            ffmpeg\n",
    "            .output(video_input, audio_input, f\"staticAudioVid/{video_index}.mp4\",\n",
    "                    vcodec='copy', acodec='aac', t=chronological_dialogue_duration[video_index])\n",
    "            .overwrite_output()  # Overwrite output file if it exists.\n",
    "        )\n",
    "\n",
    "        # Run the FFmpeg command.\n",
    "        ffmpeg.run(out)\n",
    "        print(f\"Successfully created staticAudioVid/{currentVidIdx}.mp4\")\n",
    "        return f\"staticAudioVid/{currentVidIdx}.mp4\";\n",
    "\n",
    "    except ffmpeg.Error as e:\n",
    "        # If an error occurs, decode and print the stderr.\n",
    "        error_message = e.stderr.decode('utf-8') if e.stderr else str(e)\n",
    "        print(\"An error occurred while combining audio and video:\")\n",
    "        print(error_message)\n",
    "\n",
    "editor_tools = [combine_video_dialogue]\n",
    "editor_tool_node = ToolNode(editor_tools)\n",
    "editor_worker = editor_llm.bind_tools(editor_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76812a2d-d0f4-4679-91e2-ed9e49ede09d",
   "metadata": {},
   "source": [
    "## RUNNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcbeb42-ff84-4b08-89ac-e84f2471fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor\n",
      "traveling\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "storyboarding\n",
      "STORY ANALYSIS\n",
      "------------------------------\n",
      "Inside the Montague Museum, once a grand family home akin to the opulent Hearst Castle, the scene unfolds in a lavish, tour-driven environment. The grandeur of the museum, with its high ceilings and ornate decor, provides a stark contrast to the intense argument taking place between two central characters: Elizabeth Montague and Ethan. Elizabeth Montague, a woman of striking intensity and authority, is locked in a heated exchange with Ethan, a contrasting figure whose rebellious spirit challenges her commands. The tension in the air is palpable as they stand amidst the whispers of history, surrounded by artifacts and the echoes of past grandeur. This scene delves into the themes of secrecy, power, and rebellion, as Elizabeth tries to maintain control over a potentially explosive secret while Ethan seeks to expose the truth, regardless of the consequences.\n",
      "CHARACTERS\n",
      "------------------------------\n",
      "Elizabeth Montague is a woman in her late thirties, elegantly dressed in a tailored suit that hints at her status and power. Her presence is commanding, and her eyes hold an intensity that speaks of years spent maintaining control over her family’s legacy. Elizabeth is accustomed to managing crises with a firm hand, and her anger in this scene is a testament to the severity of the secret she wants to protect. Her character is complex, driven by a deep-seated need to uphold the family’s reputation at any cost, yet haunted by the fear that her carefully controlled world could unravel at the hands of someone like Ethan.\n",
      "\n",
      "Ethan, in his mid-twenties, is the antithesis of Elizabeth. He carries a casual yet defiant air, dressed in a way that suggests he is unconcerned with the trappings of wealth and status. His character is driven by a strong sense of justice and truth, which puts him at odds with Elizabeth’s desire for secrecy. Ethan’s rebellious nature is reflected in his body language—he is unafraid to stand his ground, even in the face of Elizabeth’s formidable presence. His determination to reveal the truth is fueled by a personal conviction that integrity should triumph over propriety.\n",
      "BACKGROUND\n",
      "------------------------------\n",
      "The museum's grandeur is evident in its lavish halls, filled with priceless artifacts and paintings that whisper stories of a bygone era. The setting is both awe-inspiring and imposing, reflecting the Montague family's illustrious past and the weight of the legacy Elizabeth seeks to protect.\n",
      "AUDIO\n",
      "------------------------------\n",
      "Elizabeth's voice is sharp and incisive, carrying a tone of authority and desperation as she demands secrecy. Her inflection is controlled, yet there is an underlying tremor of anxiety. Ethan's voice contrasts hers, resonating with youthful defiance and sincerity. He speaks with conviction and an air of disbelief, emphasizing his refusal to be silenced.\n",
      "DIALOGUE\n",
      "------------------------------\n",
      "Elizabeth’s lines should be delivered with a mix of authority and desperation, emphasizing her need for secrecy. Her final words should carry a sharp, urgent tone, highlighting the severity of the situation. Ethan’s responses should convey disbelief and resolute defiance, with an emphasis on his determination to reveal the truth.\n",
      "Number of characters\n",
      "------------------------------\n",
      "2\n",
      "Pure Dialogue\n",
      "------------------------------\n",
      "Elizabeth: You know you can’t tell a soul, right? (Authority) You know you can’t tell a soul, right? Ethan: Are you kidding? I’m going to tell everyone. You can’t hide this. (Defiance) Are you kidding? I’m going to tell everyone. You can’t hide this.\n",
      "Unique Dialogue Instances\n",
      "------------------------------\n",
      "2\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "generating char profile for: Elizabeth Montague\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "Dreaming, state:completed\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/36cb3969-be15-4378-83d3-bcdee0e3d18f/887fd56b-e0ff-4c87-93c7-993faf65062c_result.jpg\n",
      "Image generated in charRef/0.jpg\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "generating char profile for: Ethan\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:completed\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/b3e3e481-3a4b-41c7-ad7b-70045c5c96f0/a2f085b2-d06b-4fa8-af8b-9eff9c69d29c_result.jpg\n",
      "Image generated in charRef/1.jpg\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "supervisor\n",
      "traveling\n",
      "audio worker\n",
      "check audio tool call\n",
      "attempted dialogue: You know you can’t tell a soul, right? (Authority) You know you can’t tell a soul, right?\n",
      "attempted idx: 2\n",
      "attempted dialogue: Are you kidding? I’m going to tell everyone. You can’t hide this. (Defiance) Are you kidding? I’m going to tell everyone. You can’t hide this.\n",
      "attempted idx: 0\n",
      "duration: 5.328938\n",
      "characterSpeaking: Elizabeth\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/36cb3969-be15-4378-83d3-bcdee0e3d18f/887fd56b-e0ff-4c87-93c7-993faf65062c_result.jpg\n",
      "waiting for dialogue gen process to finish first\n",
      "duration: 9.613063\n",
      "characterSpeaking: Ethan\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/b3e3e481-3a4b-41c7-ad7b-70045c5c96f0/a2f085b2-d06b-4fa8-af8b-9eff9c69d29c_result.jpg\n",
      "audio worker\n",
      "check audio tool call\n",
      "supervisor\n",
      "traveling\n",
      "video worker\n",
      "check video tool call\n",
      "START KEYFRAME PROMPT: Elizabeth Montague stands in a grand, opulent room within the Montague Museum, reflecting the blend of luxury and history. Her intense gaze captures the visitor's attention as she prepares to confront Ethan. The start keyframe should depict her standing authoritatively amidst the grandeur, with visible tension in her posture.\n",
      "END KEYFRAME PROMPT: Elizabeth Montague, with slightly softened yet firm gaze, stands amidst the opulent and historical backdrop of the Montague Museum. The dialogue with Ethan has reached its peak, and she is resolute, having stood her ground. The end keyframe should feature her in a moment of stillness yet determination in an imposing room filled with artifacts.\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreamingEnd Keyframe Dreaming, state:dreaming\n",
      "\n",
      "FAILED IMG\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "FAILED IMG\n",
      "video worker\n",
      "check video tool call\n",
      "START KEYFRAME PROMPT: Elizabeth Montague is positioned within the grand halls of the Montague Museum. The image captures her intense gaze and determined posture as she stands surrounded by ornate artifacts and high ceilings, reflecting both luxury and the weight of family legacy.\n",
      "END KEYFRAME PROMPT: Elizabeth concludes her argument in the grand setting of the Montague Museum, her expression softening slightly as she maintains her stance, emphasized by the elegant and historical surroundings.\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:completed\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "start_keyframe_image_url: https://storage.cdn-luma.com/dream_machine/1a4f1703-de02-436c-b367-b091b5a79348/07a2acd1-65e3-4ced-9db2-0eeb2a65e7bc_result.jpg\n",
      "End Keyframe Dreaming, state:completed\n",
      "Image generated in staticStartKeyFrame/0.jpg\n",
      "end_keyframe_image_url: https://storage.cdn-luma.com/dream_machine/66d38e8a-c71a-4df0-bfc1-b589e0c698c8/673899cc-80b4-451c-abfb-aa4804d735de_result.jpg\n",
      "Image generated in staticEndKeyFrame/0.jpg\n",
      "video worker\n",
      "check video tool call\n",
      "VIDEOWORKER PROMPT: 9s\n",
      "------------------------------\n",
      "The video captures an intense confrontation between Elizabeth Montague and Ethan in the opulent Montague Museum. Elizabeth, with her commanding presence, demands secrecy about the family affair, while Ethan rebels against her authority, expressing his intention to reveal the secret. The video showcases Elizabeth’s authoritative demeanor and Ethan’s youthful defiance amidst the lavish and historical backdrop of the museum, reflecting the tension and high stakes of their interaction.\n",
      "generating vid\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "FAILED VID\n",
      "video worker\n",
      "check video tool call\n",
      "VIDEOWORKER PROMPT: 9s\n",
      "------------------------------\n",
      "In the grand, ornamental halls of the Montague Museum, Elizabeth Montague engages in a heated exchange with Ethan. Elizabeth, with her intense gaze and commanding posture, demands secrecy concerning the family legacy. In contrast, the rebellious and defiant Ethan challenges her authority, determined to disclose the truth. The video highlights their emotional confrontation and power struggle against the backdrop of the museum's historic wealth, symbolizing the clash between secrecy and revelation.\n",
      "generating vid\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "FAILED VID\n",
      "video worker\n",
      "check video tool call\n",
      "doesn't want to use tool\n",
      "supervisor\n",
      "traveling\n",
      "editor worker\n",
      "check editor tool call\n",
      "Combining vid and dialogue\n",
      "editor worker\n",
      "check editor tool call\n",
      "supervisor\n",
      "traveling\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m generatingVid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Use the agent\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGenerate a *STORYBOARD*, *AUDIO*, *VIDEO* FOR ALL CHARACTERS THEN *EDIT* THE VIDEO AND AUDIO FILES TOGETHER for the following scene: INT. MONTAGUE MUSEUM – MODERN DAYThe once-grand family home, now a tour-driven Hearst Castle-like museum.ELIZABETH MONTAGUE, intense and angry, argues with Ethan.ELIZABETHYou know you can’t tell a soul,right?ETHANAre you kidding? I’m going to telleveryone. You can’t hide this.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthread_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mD:\\Hackathoncool\\CreativeControl\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2069\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2068\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2069\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2070\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2071\u001b[0m     config,\n\u001b[0;32m   2072\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2073\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2074\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2075\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2076\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2078\u001b[0m ):\n\u001b[0;32m   2079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2080\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mD:\\Hackathoncool\\CreativeControl\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1744\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1736\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[0;32m   1737\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1738\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1742\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[0;32m   1743\u001b[0m     )\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[1;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "members = [\"video_worker\",\"storyboard_worker\",\"audio_worker\",\"editor_worker\"] #\"editor\"\n",
    "supervisor_options = members + [END]\n",
    "visitedAudioWorker = False\n",
    "class Supervisor_Router(TypedDict):\n",
    "    \"\"\"Worker to route to next to fulfill the user's request. If no workers are needed, route to END.\"\"\"\n",
    "\n",
    "    next: Literal[*supervisor_options]\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def go_next(state: MessagesState) -> Literal[*supervisor_options]:\n",
    "    global visitedAudioWorker\n",
    "    print(f\"traveling\")\n",
    "    if(state[\"next\"]==\"video_worker\" and not visitedAudioWorker):\n",
    "        print(\"forced audio travel\")\n",
    "        visitedAudioWorker = True\n",
    "        return \"audio_worker\"\n",
    "    return state[\"next\"]\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_supervisor(state: MessagesState):\n",
    "    print(\"supervisor\")\n",
    "    messages = state[\"messages\"]\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a supervisor of a short film project and am in charge of a team of 4. You can delegate relevant tasks to any of these members: {members}.\"\n",
    "        \"Bob the video_worker is capable of generating high fidelity videos, but requires clear contextual information. Ryan the audio_worker is able to generate character\"\n",
    "        \"dialogue audio clips. Steve the storyboard_worker is able to analyze a given input script and break it down into fine details and generate character profiles.\"\n",
    "        \"\\nYou should ALWAYS ensure Steve has generated a STORYBOARD and character profiles for ALL characters analyzed in the storyboard tool response before anything else.\"\n",
    "        \"\\nIMPORTANT!!! MAKE SURE Ryan generates audio clips BEFORE Bob generates video clips. Generate a step-by-step plan from the following prompt and act on it.\"\n",
    "        \"\\nEXAMPLE PLAN(FOLLOW): STEVE STORYBOARD -> STEVE CHARACTER PROFILES -> RYAN AUDIO CLIPS -> BOB VIDEO GENERATION -> DAVE EDITOR\"\n",
    "        \"When responding, please output a JSON object that follows this schema:\\n\"\n",
    "        '{ \"next\": one of the allowed values: ' + \", \".join(supervisor_options) + \" }\\n\"\n",
    "        \"If no further workers are needed, output 'END' as the next step.\"\n",
    "    }\n",
    "    response = supervisor_llm.with_structured_output(Supervisor_Router).invoke(messages)\n",
    "    # Wrap the response in a valid message format\n",
    "    structured_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Routing to {response['next']}\"\n",
    "    }\n",
    "    return {\"messages\": [structured_message], \"next\": response[\"next\"]}\n",
    "\n",
    "def call_video_worker(state: MessagesState):\n",
    "    print(\"video worker\")\n",
    "    messages = state['messages']\n",
    "    extraStr = \"video being generated before generating another. [Use Unique Prompts].\"\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Bob, a video worker. Process the given request accordingly. Note that you can only generate 5 second or 9 second videos.\"\n",
    "        \"For each distinct dialogue instance, FIRST generate a START keyframe THEN an END keyframe THEN LASTLY a VIDEO using the three tools you are given ONE TIME\"\n",
    "    }\n",
    "    response = video_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_storyboard_worker(state: MessagesState):\n",
    "    print(\"storyboard worker\")\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are Steve, a storyboarder. Process the given request accordingly. You have access to two tools: a storyboard generator and a character profile\\\n",
    "        generator. Generate only ONE character profile at a time. You should pass in all character results from the storyboard generator into the character profile generator.\"\n",
    "    }\n",
    "    response = storyboard_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_audio_worker(state: MessagesState):\n",
    "    global visitedAudioWorker\n",
    "    global fullDialogueString\n",
    "    print(\"audio worker\")\n",
    "    visitedAudioWorker=True\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Ryan, a character dialogue generator. Process the given request accordingly. You have access to one tool\"\n",
    "        \"which generates a dialogue audio clip for a SINGLE character talking ONLY AS PER THE CHRONOLOGICAL OREDER of the storyboard generated by STEVE.\"\n",
    "        \"Check if you have already generated a piece of dialogue. If you have, don't generate it again.\"\n",
    "        \"Parse this string once for each instance with a character talking generate audio:\"+ fullDialogueString\n",
    "    }\n",
    "    response = audio_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "def call_editor_worker(state: MessagesState):\n",
    "    print(\"editor worker\")\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are Dave, a video editor. Process the given request accordingly. You have access to one tool that allows you to combine an mp3 and mp4 file and cut the length\\\n",
    "        so that the video will end when the audio ends. CONTINUE combining until you run out of indices to plug into the tool. REMEMBER that the max index is the number of distinct dialogue instances minus 1\"\n",
    "    }\n",
    "    response = editor_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def check_video_tool_calls(state: MessagesState) -> Literal[\"video_tools\",\"supervisor\"]:\n",
    "    print(\"check video tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"video_tools\"\n",
    "    print(\"doesn't want to use tool\")\n",
    "    return \"supervisor\"\n",
    "\n",
    "def check_storyboard_tool_calls(state: MessagesState) -> Literal[\"storyboard_tools\",\"supervisor\"]:\n",
    "    print(\"check storyboard tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"storyboard_tools\"\n",
    "    return \"supervisor\"\n",
    "    \n",
    "def check_audio_tool_calls(state: MessagesState) -> Literal[\"audio_tools\",\"supervisor\"]:\n",
    "    print(\"check audio tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"audio_tools\"\n",
    "    return \"supervisor\"\n",
    "\n",
    "def check_editor_tool_calls(state: MessagesState) -> Literal[\"editor_tools\",\"supervisor\"]:\n",
    "    print(\"check editor tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"editor_tools\"\n",
    "    return \"supervisor\"\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"supervisor\", call_supervisor)\n",
    "workflow.add_node(\"video_worker\", call_video_worker)\n",
    "workflow.add_node(\"video_tools\", video_tool_node)\n",
    "workflow.add_node(\"storyboard_worker\", call_storyboard_worker)\n",
    "workflow.add_node(\"storyboard_tools\", storyboard_tool_node)\n",
    "workflow.add_node(\"audio_worker\", call_audio_worker)\n",
    "workflow.add_node(\"audio_tools\", audio_tool_node)\n",
    "workflow.add_node(\"editor_worker\", call_editor_worker)\n",
    "workflow.add_node(\"editor_tools\", editor_tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    go_next,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"video_worker\",\n",
    "    check_video_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"storyboard_worker\",\n",
    "    check_storyboard_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"audio_worker\",\n",
    "    check_audio_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"editor_worker\",\n",
    "    check_editor_tool_calls,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"video_tools\", 'video_worker')\n",
    "workflow.add_edge(\"storyboard_tools\",\"storyboard_worker\");\n",
    "workflow.add_edge(\"audio_tools\",\"audio_worker\");\n",
    "workflow.add_edge(\"editor_tools\",\"editor_worker\");\n",
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "generatingImg = False\n",
    "generatingVid = False\n",
    "# Use the agent\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Generate a *STORYBOARD*, *AUDIO*, *VIDEO* FOR ALL CHARACTERS THEN *EDIT* THE VIDEO AND AUDIO FILES TOGETHER for the following scene: INT. MONTAGUE MUSEUM – MODERN DAYThe once-grand family home, now a tour-driven Hearst Castle-like museum.ELIZABETH MONTAGUE, intense and angry, argues with Ethan.ELIZABETHYou know you can’t tell a soul,right?ETHANAre you kidding? I’m going to telleveryone. You can’t hide this.\"}]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361d425-25ad-4987-b386-312e4a09d0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
