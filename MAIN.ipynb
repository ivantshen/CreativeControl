{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a0b91f-1ab1-423d-890d-3c0f4e3c6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "site.addsitedir('Lib/site-packages')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import getpass\n",
    "import os\n",
    "import requests\n",
    "import ffmpeg\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import uuid\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from lumaai import LumaAI\n",
    "from typing import Literal, TypedDict\n",
    "from elevenlabs.client import ElevenLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7c9450-b7f9-49e2-846a-03b8bead1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LUMA\n",
    "client = LumaAI(\n",
    "    auth_token=os.environ.get(\"LUMAAI_API_KEY\"),\n",
    ")\n",
    "#OPENAI\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "#ELEVENLABS\n",
    "elevenlabs_client = ElevenLabs(\n",
    "  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "#LANGCHAIN llms\n",
    "supervisor_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "video_gen_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "storyboard_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "audio_gen_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "editor_llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "generatingVid = False\n",
    "fullDialogueString = \"\"\n",
    "chronological_dialogue_duration = []\n",
    "chronological_dialogue_mp3_path = []\n",
    "chronological_video_mp4_path = []\n",
    "chrono_character=[]\n",
    "character_profiles_chrono=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13dc3f3-a4e8-4fd0-9ffc-b7244c81da71",
   "metadata": {},
   "source": [
    "## TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bd710-06c9-432d-9602-c38a3f7a5fba",
   "metadata": {},
   "source": [
    "### Video Worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6845f-1e09-46a7-ba7d-e944643b671c",
   "metadata": {},
   "source": [
    "#SCHEMAS\n",
    "class video_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a video using text input and returns a filepath to the video\"\"\"\n",
    "    vid_prompt: str = Field(..., description=\"The textual prompt used in generating the video\")\n",
    "    use_9s: bool = Field(..., description=\"Whether to generate a 9 second long video. If set to FALSE, this will generate a 5 second long video\")\n",
    "\n",
    "@tool(\"video_gen_tool\",args_schema=video_gen_schema)   \n",
    "def generate_vid(vid_prompt: str, use_9s: bool) -> str:\n",
    "    global generatingVid\n",
    "    if(generatingVid):\n",
    "        return \"Failed to generate video, there is currently another video being generated.\"\n",
    "    dur = \"5s\"\n",
    "    if(use_9s):\n",
    "        dur = \"9s\"\n",
    "    print(f\"VIDEOWORKER PROMPT: {dur}\\n------------------------------\\n\" + vid_prompt)\n",
    "    generatingVid = True\n",
    "    generation = client.generations.create(\n",
    "        prompt=vid_prompt,\n",
    "        model=\"ray-2\",\n",
    "        resolution=\"720p\",\n",
    "        duration=dur\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating vid\")\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        raise RuntimeError(f\"Generation failed: {generation.failure_reason}\")\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(5)\n",
    "    video_url = generation.assets.video\n",
    "    # download the video\n",
    "    response = requests.get(video_url, stream=True)\n",
    "    with open(f'staticVid1/{generation.id}.mp4', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Video generated in staticVid1/{generation.id}.mp4\")\n",
    "    generatingVid = False\n",
    "    return f\"Video generated in staticVid1/{generation.id}.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9958d677-52aa-48e2-a95b-5e2278d3254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCHEMAS\n",
    "start_keyframes = [None] * 50\n",
    "end_keyframes = [None] * 50\n",
    "video_worker_generating = False\n",
    "class start_keyframe_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a start keyframe image using text input and returns a filepath to the image\"\"\"\n",
    "    start_keyframe_prompt: str = Field(..., description=\"200 words about start keyframe details\")\n",
    "    start_keyframe_index: int = Field(..., description=\"The index of the keyframe to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"start_keyframe_gen_tool\",args_schema=start_keyframe_gen_schema)   \n",
    "def generate_start_keyframe(start_keyframe_prompt:str,start_keyframe_index: int)->str:\n",
    "    global character_profiles_chrono\n",
    "    global start_keyframes\n",
    "    global video_worker_generating\n",
    "    print(f\"START KEYFRAME PROMPT: \"+ start_keyframe_prompt)\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(start_keyframe_index>=numClipsToGen):\n",
    "        return \"Invalid start_keyframe_index, try again.\"\n",
    "    while(video_worker_generating):\n",
    "        time.sleep(5)\n",
    "        print(\"paused start keyframe execution, worker in use\")\n",
    "    video_worker_generating = True\n",
    "    start_keyframe_generation = client.generations.image.create(\n",
    "        prompt=start_keyframe_prompt,\n",
    "        image_ref=[\n",
    "          {\n",
    "            \"url\": character_profiles_chrono[start_keyframe_index],\n",
    "            \"weight\": 0.7\n",
    "          }\n",
    "        ]\n",
    "    )\n",
    "    start_keyframe_completed = False\n",
    "    while not start_keyframe_completed:\n",
    "      start_keyframe_generation = client.generations.get(id=start_keyframe_generation.id)\n",
    "      if start_keyframe_generation.state == \"completed\":\n",
    "        start_keyframe_completed = True\n",
    "      elif start_keyframe_generation.state == \"failed\":\n",
    "        print(\"FAILED IMG\")\n",
    "        video_worker_generating = False\n",
    "        return f\"Generation failed: {start_keyframe_generation.failure_reason} Attempt a regeneration.\"\n",
    "      print(\"Start Keyframe Dreaming, state:\" + start_keyframe_generation.state)\n",
    "      time.sleep(2)\n",
    "    start_keyframe_image_url = start_keyframe_generation.assets.image\n",
    "    print(\"start_keyframe_image_url: \" +start_keyframe_image_url)\n",
    "    start_keyframes[start_keyframe_index] = start_keyframe_image_url\n",
    "    start_keyframe_response = requests.get(start_keyframe_image_url, stream=True)\n",
    "    with open(f'staticStartKeyFrame/{start_keyframe_index}.jpg', 'wb') as file:\n",
    "        file.write(start_keyframe_response.content)\n",
    "    print(f\"Image generated in staticStartKeyFrame/{start_keyframe_index}.jpg\")\n",
    "    video_worker_generating = False\n",
    "    return f'staticStartKeyFrame/{start_keyframe_index}.jpg'\n",
    "\n",
    "\n",
    "class end_keyframe_gen_schema(BaseModel):\n",
    "    \"\"\"Generates an end keyframe image using text input and returns a filepath to the image\"\"\"\n",
    "    end_keyframe_prompt: str = Field(..., description=\"200 words about end keyframe details\")\n",
    "    end_keyframe_index: int = Field(..., description=\"The index of the keyframe to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "@tool(\"end_keyframe_gen_tool\",args_schema=end_keyframe_gen_schema)   \n",
    "def generate_end_keyframe(end_keyframe_prompt:str,end_keyframe_index: int)->str:\n",
    "    global character_profiles_chrono\n",
    "    global end_keyframes\n",
    "    global video_worker_generating\n",
    "    print(f\"END KEYFRAME PROMPT: \"+ end_keyframe_prompt)\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(end_keyframe_index>=numClipsToGen):\n",
    "        return \"Invalid end_keyframe_index, try again.\"\n",
    "    while(video_worker_generating):\n",
    "        time.sleep(5)\n",
    "        print(\"paused start keyframe execution, worker in use\")\n",
    "    video_worker_generating = True\n",
    "    end_keyframe_generation = client.generations.image.create(\n",
    "        prompt=end_keyframe_prompt,\n",
    "        image_ref=[\n",
    "          {\n",
    "            \"url\": character_profiles_chrono[end_keyframe_index],\n",
    "            \"weight\": 0.7\n",
    "          }\n",
    "        ]\n",
    "    )\n",
    "    end_keyframe_completed = False\n",
    "    while not end_keyframe_completed:\n",
    "      end_keyframe_generation = client.generations.get(id=end_keyframe_generation.id)\n",
    "      if end_keyframe_generation.state == \"completed\":\n",
    "        end_keyframe_completed = True\n",
    "      elif end_keyframe_generation.state == \"failed\":\n",
    "        print(\"FAILED IMG\")\n",
    "        video_worker_generating = False\n",
    "        return f\"Generation failed: {end_keyframe_generation.failure_reason} Attempt a regeneration.\"\n",
    "      print(\"End Keyframe Dreaming, state:\" + end_keyframe_generation.state)\n",
    "      time.sleep(2)\n",
    "    end_keyframe_image_url = end_keyframe_generation.assets.image\n",
    "    print(\"end_keyframe_image_url: \" +end_keyframe_image_url)\n",
    "    end_keyframes[end_keyframe_index] = end_keyframe_image_url\n",
    "    end_keyframe_response = requests.get(end_keyframe_image_url, stream=True)\n",
    "    with open(f'staticEndKeyFrame/{end_keyframe_index}.jpg', 'wb') as file:\n",
    "        file.write(end_keyframe_response.content)\n",
    "    print(f\"Image generated in staticEndKeyFrame/{end_keyframe_index}.jpg\")\n",
    "    video_worker_generating = False\n",
    "    return f'staticEndKeyFrame/{end_keyframe_index}.jpg'\n",
    "\n",
    "class video_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a video using text input and returns a filepath to the video\"\"\"\n",
    "    video_index: int = Field(..., description=\"The index of the clip to generate. MUST be less than the number of distinct dialogue instances\")\n",
    "    vid_prompt: str = Field(..., description=\"200 words about details of the specified character talking\")\n",
    "    use_9s: bool = Field(..., description=\"Whether to generate a 9 second long video. If set to FALSE, this will generate a 5 second long video\")\n",
    "@tool(\"video_gen_tool\",args_schema=video_gen_schema)   \n",
    "def generate_vid(video_index: int, vid_prompt: str, use_9s: bool) -> str:\n",
    "    global character_profiles_chrono\n",
    "    global chrono_character\n",
    "    global start_keyframes\n",
    "    global end_keyframes\n",
    "    global video_worker_generating\n",
    "    numClipsToGen = len(character_profiles_chrono)\n",
    "    if(video_index>=numClipsToGen):\n",
    "        return \"Invalid video_index, try again.\"\n",
    "    while(video_worker_generating):\n",
    "        time.sleep(5)\n",
    "        print(\"paused start keyframe execution, worker in use\")\n",
    "    video_worker_generating = True\n",
    "    dur = \"5s\"\n",
    "    if(use_9s):\n",
    "        dur = \"9s\"\n",
    "    print(f\"VIDEOWORKER PROMPT: {dur}\\n------------------------------\\n\" + vid_prompt)\n",
    "    generation = client.generations.create(\n",
    "        prompt=vid_prompt,\n",
    "        keyframes={\n",
    "          \"frame0\": {\n",
    "            \"type\": \"image\",\n",
    "            \"url\": start_keyframes[video_index]\n",
    "          },\n",
    "          \"frame1\": {\n",
    "            \"type\": \"image\",\n",
    "            \"url\": end_keyframes[video_index]\n",
    "          }\n",
    "        },\n",
    "        model=\"ray-2\",\n",
    "        resolution=\"720p\",\n",
    "        duration=dur\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating vid\")\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        print(\"FAILED VID\")\n",
    "        video_worker_generating = False\n",
    "        return (f\"Generation failed: {generation.failure_reason}\")\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(5)\n",
    "    video_url = generation.assets.video\n",
    "    # download the video\n",
    "    response = requests.get(video_url, stream=True)\n",
    "    with open(f'staticVid1/{video_index}.mp4', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Video generated in staticVid1/{video_index}.mp4\")\n",
    "    video_worker_generating = False\n",
    "    return f\"Video generated in staticVid1/{video_index}.mp4\"\n",
    "\n",
    "#class extend_video_schema(BaseModel):\n",
    "video_tools = [generate_vid,generate_start_keyframe,generate_end_keyframe]\n",
    "video_tool_node = ToolNode(video_tools)\n",
    "video_worker = video_gen_llm.bind_tools(video_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b5b6e-939f-4b35-b636-9e8842e04e44",
   "metadata": {},
   "source": [
    "### Storyboard Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91838bf-2756-46e3-955b-8689016710fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCHEMA\n",
    "class step_by_step_output_schema(BaseModel):\n",
    "    \"\"\"Prints a storyboard for the user to view in string format\"\"\"\n",
    "    story_description: str = Field(..., description=\"A 200-250 word description of the story\")\n",
    "    character_details: str = Field(..., description=\"A 200-300 word description of each character in the scene\")\n",
    "    background_details: str = Field(..., description=\"A 50-100 description of the scene\")\n",
    "    auditory_details: str = Field(..., description=\"A 50-100 description of the voice profile of each character\")\n",
    "    dialogue_details: str = Field(..., description=\"The parts of the dialogue that require enunciation and emotion\")\n",
    "    num_characters: int = Field(..., description=\"The number of unique characters present in this scene\")\n",
    "    pure_dialogue: str = Field(..., description=\"The extracted dialogue from the input scene with character name appended before. Within the dialogue, indicate dialogue details that require enunciation and emotion with descriptors in parentheses Format: {CharacterName}: {Dialogue (Enunciation) Dialogue Detail}\")\n",
    "    dialogue_instances: int = Field(..., description=\"The number of distinct dialogue instances present in this scene\")\n",
    "charProfileQueue = 0\n",
    "dialogueQueue = 0\n",
    "@tool(\"storyboard_tool\",args_schema=step_by_step_output_schema)\n",
    "def generate_storyboard(story_description: str,character_details: str,background_details: str,auditory_details: str,dialogue_details: str, num_characters: int,pure_dialogue: str, dialogue_instances: int) -> str:\n",
    "    global fullDialogueString\n",
    "    global charProfileQueue\n",
    "    global dialogueQueue\n",
    "    print(\"storyboarding\")\n",
    "    temp_storyboard = \"STORY ANALYSIS\\n------------------------------\\n\"+story_description+\"\\nCHARACTERS\\n------------------------------\\n\"+character_details\n",
    "    temp_storyboard+=\"\\nBACKGROUND\\n------------------------------\\n\"+background_details+\"\\nAUDIO\\n------------------------------\\n\"+auditory_details+\"\\nDIALOGUE\\n------------------------------\\n\"\n",
    "    temp_storyboard+=dialogue_details+\"\\nNumber of characters\\n------------------------------\\n\"+ str(num_characters)+\"\\nPure Dialogue\\n------------------------------\\n\" + pure_dialogue\n",
    "    temp_storyboard+=\"\\nUnique Dialogue Instances\\n------------------------------\\n\"+ str(dialogue_instances)\n",
    "    print(temp_storyboard)\n",
    "    fullDialogueString = pure_dialogue\n",
    "    charProfileQueue = num_characters\n",
    "    dialogueQueue = dialogue_instances\n",
    "    return temp_storyboard\n",
    "\n",
    "class character_profile_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a character profile for a character and returns the url\"\"\"\n",
    "    character_name: str = Field(..., description=\"The name of the character, taken from the script\")\n",
    "    character_details: str = Field(..., description=\"A 200-300 word description of the character in the scene, taken from the script\")\n",
    "charProfileIdx = 0\n",
    "generatingCharProfile = False\n",
    "@tool(\"character_profile_tool\",args_schema=character_profile_gen_schema)\n",
    "def generate_character_profile(character_name: str, character_details: str)->str:\n",
    "    global charProfileQueue\n",
    "    global charProfileIdx\n",
    "    global generatingCharProfile\n",
    "    if(charProfileQueue<=0):\n",
    "        return \"Already generated character profiles!\"\n",
    "    while(generatingCharProfile):\n",
    "        time.sleep(5)\n",
    "        print(\"paused char profile gen, waiting for first generation to complete\")\n",
    "    generatingCharProfile = True\n",
    "    generation = client.generations.image.create(\n",
    "      prompt=\"Generate a hyperrealistic, front-facing portrait \\\n",
    "      The image should feature perfectly even, diffused lighting that completely\\\n",
    "      eliminates any shadows on the face. Use a direct, center-camera angle against a neutral,\\\n",
    "      unobtrusive background to ensure absolute consistency. Focus on lifelike details with natural \\\n",
    "      skin textures and realistic, balanced color tones, making the portrait suitable as a reference\\\n",
    "      for video character consistency.: \"+character_name+\", \"+character_details,\n",
    "    )\n",
    "    completed = False\n",
    "    print(\"generating char profile for: \" + character_name)\n",
    "    while not completed:\n",
    "      generation = client.generations.get(id=generation.id)\n",
    "      if generation.state == \"completed\":\n",
    "        completed = True\n",
    "      elif generation.state == \"failed\":\n",
    "        generatingCharProfile = False\n",
    "        print(\"FAILED IMG\")\n",
    "        return f\"Generation failed: {generation.failure_reason} Attempt this again.\"\n",
    "      print(\"Dreaming, state:\" + generation.state)\n",
    "      time.sleep(2)\n",
    "    image_url = generation.assets.image\n",
    "    print(\"image_url: \" +image_url)\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    with open(f'charRef/{charProfileIdx}.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Image generated in charRef/{charProfileIdx}.jpg\")\n",
    "    charProfileQueue-=1\n",
    "    charProfileIdx+=1\n",
    "    generatingCharProfile = False\n",
    "    return f\"Profile generated: {image_url}\"\n",
    "\n",
    "storyboard_tools = [generate_storyboard,generate_character_profile]\n",
    "storyboard_tool_node = ToolNode(storyboard_tools)\n",
    "storyboard_worker = storyboard_llm.bind_tools(storyboard_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf12443-0259-452e-bf07-27d0d57f9593",
   "metadata": {},
   "source": [
    "### Audio Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd454937-f86f-4a39-84fa-e8671ce437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dialogue_gen_schema(BaseModel):\n",
    "    \"\"\"Generates a dialogue mp3 clip and returns the filepath to the audio\"\"\"\n",
    "    character_name: str = Field(..., description=\"The name of the character talking\")\n",
    "    character_profile: str = Field(..., description=\"The link to the generated character profile of the character who is talking. Format: {https://storage.cdn-luma.com/dream_machine/../.._result.jpg}\")\n",
    "    dialogue_text: str = Field(..., description=\"The dialogue text of a SINGLE character for a particular scene.\")\n",
    "    voice_idx: int = Field(..., description=\"Out of the following voice ids, choose the index of the one that best suits the character who is speaking:\\\n",
    "    [{UgBBYS2sOqTuMpoF3BR0: Male younger and nervous},{N2lVS1w4EtoT3dr4eOWO: Male booming and imposing},\\\n",
    "    {21m00Tcm4TlvDq8ikWAM: Female elegant and slightly teasing},{UgBBYS2sOqTuMpoF3BR0: Male, middle-aged, natural, casual and smooth},\\\n",
    "    {19STyYD15bswVz51nqLf: Female, trustworhy, warm, middle-aged},{gOkFV1JMCt0G0n9xmBwV: Male, middle-older, controlled, honest, respected}]\")\n",
    "audioIdx = 0\n",
    "generatingDialogueAudio = False\n",
    "@tool(\"dialogue_gen_tool\",args_schema=dialogue_gen_schema)\n",
    "def generate_dialogue(character_name: str,character_profile: str,dialogue_text: str, voice_idx: int) -> str:\n",
    "    global chronological_dialogue_duration\n",
    "    global chronological_dialogue_mp3_path\n",
    "    global chrono_character\n",
    "    global character_profiles_chrono\n",
    "    global dialogueQueue\n",
    "    global audioIdx\n",
    "    global generatingDialogueAudio\n",
    "    print(\"attempted dialogue: \" +dialogue_text+\"\\nattempted idx: \" + str(voice_idx))\n",
    "    if(dialogueQueue<=0):\n",
    "        return \"Already generated all dialogue!\"\n",
    "    while(generatingDialogueAudio):\n",
    "        time.sleep(5)\n",
    "        print(\"waiting for dialogue gen process to finish first\")\n",
    "    generatingDialogueAudio = True\n",
    "    voice_ids = [\"29vD33N1CtxCmqQRPOHJ\",\"N2lVS1w4EtoT3dr4eOWO\",\"21m00Tcm4TlvDq8ikWAM\",\"UgBBYS2sOqTuMpoF3BR0\",\"19STyYD15bswVz51nqLf\",\"gOkFV1JMCt0G0n9xmBwV\"]\n",
    "    eleven_response = elevenlabs_client.text_to_speech.convert(\n",
    "        voice_id=voice_ids[voice_idx],\n",
    "        optimize_streaming_latency=\"0\",\n",
    "        output_format=\"mp3_44100_128\",\n",
    "        text=dialogue_text,\n",
    "        model_id=\"eleven_multilingual_v2\",\n",
    "    )\n",
    "    \n",
    "    # Save the file into the raw_audio directory\n",
    "    raw_audio_dir = \"staticAudio1\"\n",
    "    os.makedirs(raw_audio_dir, exist_ok=True)\n",
    "    file_path = os.path.join(raw_audio_dir, f\"{audioIdx}.mp3\")\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        for chunk in eleven_response:\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    duration = 0\n",
    "    try:\n",
    "        probe = ffmpeg.probe(file_path)\n",
    "        duration = float(probe['format']['duration'])\n",
    "        chronological_dialogue_mp3_path.append(file_path)\n",
    "        chronological_dialogue_duration.append(duration)\n",
    "        chrono_character.append(character_name)\n",
    "        character_profiles_chrono.append(character_profile)\n",
    "        print(\"duration: \" + str(duration) + \"\\ncharacterSpeaking: \" + character_name +\"\\ncharProfile: \" + character_profile)\n",
    "        dialogueQueue-=1\n",
    "        audioIdx+=1\n",
    "        generatingDialogueAudio = False\n",
    "        return file_path\n",
    "    except ffmpeg.Error as e:\n",
    "        generatingDialogueAudio = False\n",
    "        print(\"Error probing file:\", e.stderr)\n",
    "        raise e\n",
    "    \n",
    "audio_tools = [generate_dialogue]\n",
    "audio_tool_node = ToolNode(audio_tools)\n",
    "audio_worker = audio_gen_llm.bind_tools(audio_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a750a2f-6132-407a-b409-57ce8329ca93",
   "metadata": {},
   "source": [
    "### Editor Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1ab92a-8143-4bbe-86e4-a6c4f35a4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combine_video_dialogue_schema(BaseModel):\n",
    "    \"\"\"Combines a mp3 and mp4 into one video and cuts it accordingly and then returns the filepath to the edited file\"\"\"\n",
    "    video_index: int= Field(..., description=\"The index of the audio and video files to attempt to combine MUST be less than the number of distinct dialogue lines\")\n",
    "\n",
    "@tool(\"combine_video_dialogue_tool\",args_schema=combine_video_dialogue_schema)\n",
    "def combine_video_dialogue(video_index: int)->str:\n",
    "    global chronological_dialogue_duration\n",
    "    print(\"Combining vid and dialogue\")\n",
    "    if(video_index>=len(chronological_dialogue_duration)):\n",
    "        return f\"This index is out of bounds, there are currently {len(chronological_dialogue_duration)} elements in this array\"\n",
    "    try:\n",
    "        # Create inputs for video and audio.\n",
    "        # Using the '.video' and '.audio' attributes helps ensure we use the correct streams.\n",
    "        video_input = ffmpeg.input(f\"staticVid1/{video_index}.mp4\").video\n",
    "        audio_input = ffmpeg.input(f\"staticAudio1/{video_index}.mp3\").audio\n",
    "\n",
    "        # Build the output stream:\n",
    "        # - 'vcodec=\"copy\"' copies the video stream without re-encoding.\n",
    "        # - 'acodec=\"aac\"' encodes the audio to AAC for MP4 compatibility.\n",
    "        # - 't=mp3_duration' instructs FFmpeg to limit the output duration to the provided value.\n",
    "        out = (\n",
    "            ffmpeg\n",
    "            .output(video_input, audio_input, f\"staticAudioVid/{video_index}.mp4\",\n",
    "                    vcodec='copy', acodec='aac', t=chronological_dialogue_duration[video_index])\n",
    "            .overwrite_output()  # Overwrite output file if it exists.\n",
    "        )\n",
    "\n",
    "        # Run the FFmpeg command.\n",
    "        ffmpeg.run(out)\n",
    "        print(f\"Successfully created staticAudioVid/{currentVidIdx}.mp4\")\n",
    "        return f\"staticAudioVid/{currentVidIdx}.mp4\";\n",
    "\n",
    "    except ffmpeg.Error as e:\n",
    "        # If an error occurs, decode and print the stderr.\n",
    "        error_message = e.stderr.decode('utf-8') if e.stderr else str(e)\n",
    "        print(\"An error occurred while combining audio and video:\")\n",
    "        print(error_message)\n",
    "\n",
    "editor_tools = [combine_video_dialogue]\n",
    "editor_tool_node = ToolNode(editor_tools)\n",
    "editor_worker = editor_llm.bind_tools(editor_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76812a2d-d0f4-4679-91e2-ed9e49ede09d",
   "metadata": {},
   "source": [
    "## AGENT CALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcbeb42-ff84-4b08-89ac-e84f2471fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"video_worker\",\"storyboard_worker\",\"audio_worker\",\"editor_worker\"] #\"editor\"\n",
    "supervisor_options = members + [END]\n",
    "visitedAudioWorker = False\n",
    "class Supervisor_Router(TypedDict):\n",
    "    \"\"\"Worker to route to next to fulfill the user's request. If no workers are needed, route to END.\"\"\"\n",
    "\n",
    "    next: Literal[*supervisor_options]\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def go_next(state: MessagesState) -> Literal[*supervisor_options]:\n",
    "    global visitedAudioWorker\n",
    "    print(f\"traveling\")\n",
    "    if(state[\"next\"]==\"video_worker\" and not visitedAudioWorker):\n",
    "        print(\"forced audio travel\")\n",
    "        visitedAudioWorker = True\n",
    "        return \"audio_worker\"\n",
    "    return state[\"next\"]\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_supervisor(state: MessagesState):\n",
    "    print(\"supervisor\")\n",
    "    messages = state[\"messages\"]\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a supervisor of a short film project and am in charge of a team of 4. You can delegate relevant tasks to any of these members: {members}.\"\n",
    "        \"Bob the video_worker is capable of generating high fidelity videos, but requires clear contextual information. Ryan the audio_worker is able to generate character\"\n",
    "        \"dialogue audio clips. Steve the storyboard_worker is able to analyze a given input script and break it down into fine details and generate character profiles.\"\n",
    "        \"\\nYou should ALWAYS ensure Steve has generated a STORYBOARD and character profiles for ALL characters analyzed in the storyboard tool response before anything else.\"\n",
    "        \"\\nIMPORTANT!!! MAKE SURE Ryan generates audio clips BEFORE Bob generates video clips. Generate a step-by-step plan from the following prompt and act on it.\"\n",
    "        \"\\nEXAMPLE PLAN(FOLLOW): STEVE STORYBOARD -> STEVE CHARACTER PROFILES -> RYAN AUDIO CLIPS -> BOB VIDEO GENERATION -> DAVE EDITOR\"\n",
    "        \"When responding, please output a JSON object that follows this schema:\\n\"\n",
    "        '{ \"next\": one of the allowed values: ' + \", \".join(supervisor_options) + \" }\\n\"\n",
    "        \"If no further workers are needed, output 'END' as the next step.\"\n",
    "    }\n",
    "    response = supervisor_llm.with_structured_output(Supervisor_Router).invoke(messages)\n",
    "    # Wrap the response in a valid message format\n",
    "    structured_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Routing to {response['next']}\"\n",
    "    }\n",
    "    return {\"messages\": [structured_message], \"next\": response[\"next\"]}\n",
    "\n",
    "def call_video_worker(state: MessagesState):\n",
    "    print(\"video worker\")\n",
    "    messages = state['messages']\n",
    "    extraStr = \"video being generated before generating another. [Use Unique Prompts].\"\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Bob, a video worker. Process the given request accordingly. Note that you can only generate 5 second or 9 second videos.\"\n",
    "        \"For each distinct dialogue instance, FIRST generate a START keyframe THEN an END keyframe THEN LASTLY a VIDEO using the three tools you are given ONE TIME\"\n",
    "    }\n",
    "    response = video_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_storyboard_worker(state: MessagesState):\n",
    "    print(\"storyboard worker\")\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are Steve, a storyboarder. Process the given request accordingly. You have access to two tools: a storyboard generator and a character profile\\\n",
    "        generator. Generate only ONE character profile at a time. You should pass in all character results from the storyboard generator into the character profile generator.\"\n",
    "    }\n",
    "    response = storyboard_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_audio_worker(state: MessagesState):\n",
    "    global visitedAudioWorker\n",
    "    global fullDialogueString\n",
    "    print(\"audio worker\")\n",
    "    visitedAudioWorker=True\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Ryan, a character dialogue generator. Process the given request accordingly. You have access to one tool\"\n",
    "        \"which generates a dialogue audio clip for a SINGLE character talking ONLY AS PER THE CHRONOLOGICAL OREDER of the storyboard generated by STEVE.\"\n",
    "        \"Check if you have already generated a piece of dialogue. If you have, don't generate it again. ADDITIONALLY, Check if you have not generated a piece of dialogue, if you haven't generate it.\"\n",
    "        \"Parse this string once for each instance with a character talking generate audio:\"+ fullDialogueString\n",
    "    }\n",
    "    response = audio_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "def call_editor_worker(state: MessagesState):\n",
    "    print(\"editor worker\")\n",
    "    messages = state['messages']\n",
    "    context_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are Dave, a video editor. Process the given request accordingly. You have access to one tool that allows you to combine an mp3 and mp4 file and cut the length\\\n",
    "        so that the video will end when the audio ends. CONTINUE combining until you run out of indices to plug into the tool. REMEMBER that the max index is the number of distinct dialogue instances minus 1\"\n",
    "    }\n",
    "    response = editor_worker.invoke([context_message]+messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481743f-3ad3-4661-99b4-9552a8c84ea1",
   "metadata": {},
   "source": [
    "## TOOL NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621554cd-4966-4e35-81af-572b5295061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_video_tool_calls(state: MessagesState) -> Literal[\"video_tools\",\"supervisor\"]:\n",
    "    print(\"check video tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"video_tools\"\n",
    "    print(\"doesn't want to use tool\")\n",
    "    return \"supervisor\"\n",
    "\n",
    "def check_storyboard_tool_calls(state: MessagesState) -> Literal[\"storyboard_tools\",\"supervisor\"]:\n",
    "    print(\"check storyboard tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"storyboard_tools\"\n",
    "    return \"supervisor\"\n",
    "    \n",
    "def check_audio_tool_calls(state: MessagesState) -> Literal[\"audio_tools\",\"supervisor\"]:\n",
    "    print(\"check audio tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"audio_tools\"\n",
    "    return \"supervisor\"\n",
    "\n",
    "def check_editor_tool_calls(state: MessagesState) -> Literal[\"editor_tools\",\"supervisor\"]:\n",
    "    print(\"check editor tool call\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"editor_tools\"\n",
    "    return \"supervisor\"\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"supervisor\", call_supervisor)\n",
    "workflow.add_node(\"video_worker\", call_video_worker)\n",
    "workflow.add_node(\"video_tools\", video_tool_node)\n",
    "workflow.add_node(\"storyboard_worker\", call_storyboard_worker)\n",
    "workflow.add_node(\"storyboard_tools\", storyboard_tool_node)\n",
    "workflow.add_node(\"audio_worker\", call_audio_worker)\n",
    "workflow.add_node(\"audio_tools\", audio_tool_node)\n",
    "workflow.add_node(\"editor_worker\", call_editor_worker)\n",
    "workflow.add_node(\"editor_tools\", editor_tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    go_next,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"video_worker\",\n",
    "    check_video_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"storyboard_worker\",\n",
    "    check_storyboard_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"audio_worker\",\n",
    "    check_audio_tool_calls,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"editor_worker\",\n",
    "    check_editor_tool_calls,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"video_tools\", \"video_human_verify\")\n",
    "workflow.add_edge(\"storyboard_tools\",\"storyboard_worker\");\n",
    "workflow.add_edge(\"audio_tools\",\"audio_worker\");\n",
    "workflow.add_edge(\"editor_tools\",\"editor_worker\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5361d425-25ad-4987-b386-312e4a09d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervisor\n",
      "traveling\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "storyboarding\n",
      "STORY ANALYSIS\n",
      "------------------------------\n",
      "In the modern-day setting of the Montague Museum, housed in a previously grand family home resembling the classy yet touristic Hearst Castle, an intense confrontation unfolds between Elizabeth Montague and Ethan. Elizabeth, typically composed, is brimming with anger as she argues with Ethan over a significant, undisclosed secret. The museum, adorned with family artifacts and echoing lost glories, serves as the backdrop for this intense exchange. The tension between the characters is palpable, highlighting deeper issues of trust and betrayal between them.\n",
      "CHARACTERS\n",
      "------------------------------\n",
      "Elizabeth Montague, the central figure in her family’s history, is portrayed as a woman of strong will and determination. She exudes an air of authority and is deeply protective of her family’s secrets. Despite her fierce exterior, there lies a vulnerability within her, driven by the fear of losing control over her life's legacy. Her recent tensions with Ethan arise from his knowledge of a potentially devastating secret, which threatens to unravel the careful facade she has maintained. \n",
      "Ethan, on the other hand, is a young, vibrant individual with a relentless drive for truth and transparency. He has uncovered a truth about the Montague family that Elizabeth is desperate to keep hidden. Ethan’s character embodies a sense of righteousness, often at odds with Elizabeth's protective tendencies. He struggles with the moral implications of keeping such critical information private, leading to a heated argument with Elizabeth.\n",
      "BACKGROUND\n",
      "------------------------------\n",
      "The scene takes place in the grand hall of the Montague Museum, an opulently decorated main room with high ceilings and large windows, filled with art and artifacts. The room, despite being maintained for tourists, has a sense of antiquity and forgotten grandeur.\n",
      "AUDIO\n",
      "------------------------------\n",
      "Elizabeth Montague's voice has a regal and commanding tone, often sharp when stirred by anger. She speaks with clear enunciation, allowing her emotions to punctuate her words. Ethan’s voice, contrastingly, is youthful and earnest, with an undercurrent of frustration and urgency in his tone as he challenges Elizabeth.\n",
      "DIALOGUE\n",
      "------------------------------\n",
      "Elizabeth requires a tone filled with intensity and underlying threat when declaring 'You know you can’t tell a soul, right?' Ethan needs to convey defiance and determination in 'Are you kidding? I’m going to tell everyone. You can’t hide this.'\n",
      "Number of characters\n",
      "------------------------------\n",
      "2\n",
      "Pure Dialogue\n",
      "------------------------------\n",
      "Elizabeth: You know you can’t tell a soul, right? (Intensity) Ethan: Are you kidding? I’m going to tell everyone. You can’t hide this. (Defiance)\n",
      "Unique Dialogue Instances\n",
      "------------------------------\n",
      "2\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "generating char profile for: Elizabeth Montague\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:completed\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/52ca4938-e7dd-40cd-b060-56021ca014fb/b0d9ce8d-6f4a-4c9b-baf8-b6d8f0b06a9e_result.jpg\n",
      "Image generated in charRef/0.jpg\n",
      "paused char profile gen, waiting for first generation to complete\n",
      "generating char profile for: Ethan\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:queued\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:dreaming\n",
      "Dreaming, state:completed\n",
      "image_url: https://storage.cdn-luma.com/dream_machine/d0e37f70-3311-44aa-89b2-9ecad836e36f/7931edce-cfab-4cec-8205-62f30ada7f5d_result.jpg\n",
      "Image generated in charRef/1.jpg\n",
      "storyboard worker\n",
      "check storyboard tool call\n",
      "supervisor\n",
      "traveling\n",
      "audio worker\n",
      "check audio tool call\n",
      "attempted dialogue: You know you can’t tell a soul, right?\n",
      "attempted idx: 4\n",
      "attempted dialogue: Are you kidding? I’m going to tell everyone. You can’t hide this.\n",
      "attempted idx: 0\n",
      "duration: 2.272625\n",
      "characterSpeaking: Elizabeth\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/52ca4938-e7dd-40cd-b060-56021ca014fb/b0d9ce8d-6f4a-4c9b-baf8-b6d8f0b06a9e_result.jpg\n",
      "waiting for dialogue gen process to finish first\n",
      "duration: 4.022813\n",
      "characterSpeaking: Ethan\n",
      "charProfile: https://storage.cdn-luma.com/dream_machine/d0e37f70-3311-44aa-89b2-9ecad836e36f/7931edce-cfab-4cec-8205-62f30ada7f5d_result.jpg\n",
      "audio worker\n",
      "check audio tool call\n",
      "supervisor\n",
      "traveling\n",
      "video worker\n",
      "check video tool call\n",
      "START KEYFRAME PROMPT: The scene opens in a grand hall of the Montague Museum, reminiscent of a lavish, tourist-friendly Hearst Castle. The hall is decorated with opulent art pieces, high ceilings, and large windows that let in abundant light. Elizabeth Montague, looking intense and filled with anger, stands in the middle, confronted by Ethan. The atmosphere is tense, highlighting the old grandeur of the room contrasted by the modern-day conflict. Elizabeth's stance shows her authoritative aura as she prepares to confront Ethan.\n",
      "START KEYFRAME PROMPT: The scene opens with Ethan standing defiantly in the Montague Museum. The setting reveals opulent art and a high-ceilinged room reminiscent of Hearst Castle, embodying a forgotten grandeur. Ethan's youthful and determined expression captures his willingness to challenge Elizabeth and expose a deeply held secret. Sunlight streams through large windows, adding a dramatic effect to the imminent confrontation with Elizabeth.\n",
      "END KEYFRAME PROMPT: The scene concludes in the grand hall of the Montague Museum. The opulent decor echoes the intensity of the argument just concluded. Elizabeth Montague, visibly agitated, stands with a fierce expression, attempting to compose herself after Ethan's defiant declaration. Ethan stands firm and resolved, underscoring his determination to reveal the hidden truth. The atmosphere remains heavy with unresolved tension as the confrontation reaches a temporary pause.\n",
      "END KEYFRAME PROMPT: Ethan stands in the opulent hall of the Montague Museum, a blend of youthful defiance and determination radiating from him after his confrontation with Elizabeth. The setting retains its majestic yet tourist-driven aura with grand art pieces and towering ceilings. As the tension in the air lingers, Ethan's expression underscores his unwavering resolve to expose the truth, with Elizabeth in the background attempting to maintain her composure.\n",
      "Start Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:queued\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:queued\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "Start Keyframe Dreaming, state:completed\n",
      "End Keyframe Dreaming, state:dreaming\n",
      "End Keyframe Dreaming, state:completed\n",
      "Start Keyframe Dreaming, state:completed\n",
      "start_keyframe_image_url: https://storage.cdn-luma.com/dream_machine/f9579f73-990f-4923-aee0-f1dee949b843/452c2e5b-2871-4f27-a014-92dc62815574_result.jpg\n",
      "End Keyframe Dreaming, state:completed\n",
      "end_keyframe_image_url: https://storage.cdn-luma.com/dream_machine/ceb95400-0e00-4972-bf25-5028ee9f5a13/0b89b9c3-bc5d-426b-ab67-d928d71d900f_result.jpg\n",
      "start_keyframe_image_url: https://storage.cdn-luma.com/dream_machine/6a14100a-d7b2-4534-899e-c41ec8f67c0b/9f9b28b1-7ba9-4a12-866f-89e5b41bab5d_result.jpg\n",
      "Image generated in staticStartKeyFrame/0.jpg\n",
      "Image generated in staticEndKeyFrame/0.jpg\n",
      "Image generated in staticStartKeyFrame/1.jpg\n",
      "end_keyframe_image_url: https://storage.cdn-luma.com/dream_machine/77291eb5-8d77-46be-b071-585766d899df/0dc7e200-4776-495c-99e1-0559d3ba7ba3_result.jpg\n",
      "Image generated in staticEndKeyFrame/1.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'staticEndKeyFrame/1.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "generatingImg = False\n",
    "generatingVid = False\n",
    "# Use the agent\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Generate a *STORYBOARD*, *AUDIO*, *VIDEO* FOR ALL CHARACTERS THEN *EDIT* THE VIDEO AND AUDIO FILES TOGETHER for the following scene: INT. MONTAGUE MUSEUM – MODERN DAYThe once-grand family home, now a tour-driven Hearst Castle-like museum.ELIZABETH MONTAGUE, intense and angry, argues with Ethan.ELIZABETHYou know you can’t tell a soul,right?ETHANAre you kidding? I’m going to telleveryone. You can’t hide this.\"}]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537c2f8-d071-44c4-b345-6fc7b7805896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
